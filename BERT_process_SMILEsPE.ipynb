{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 09:48:04.845635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 09:48:04.977045: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 09:48:05.862958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2023-12-18 09:48:05.863073: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2023-12-18 09:48:05.863080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 18:44:55.022371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 18:44:56.289117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 360 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2023-12-17 18:44:56.313548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 360 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "\n",
    "    # 메모리 40% 할당\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Pretraning_data/ZINC_10M_data','rb') as file:\n",
    "    train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O=C(NC(=O)c1cccc(F)c1)O[C@H]1CCS(=O)(=O)C1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O=C(N', 'C(=O)', 'c1cccc(F)', 'c1)O', '[C@H]1CC', 'S(=O)(=O)C1'],\n",
       " ['C',\n",
       "  'n1ccnc1C',\n",
       "  '[N@@H+]1C',\n",
       "  '[C@@H](O)',\n",
       "  '[C@H](O',\n",
       "  'c2cccc(',\n",
       "  'C(=O)[O-])',\n",
       "  'c2)C1'],\n",
       " ['C',\n",
       "  'n1ccnc1C',\n",
       "  '[N@H+]1C',\n",
       "  '[C@@H](O)',\n",
       "  '[C@H](O',\n",
       "  'c2cccc(',\n",
       "  'C(=O)[O-])',\n",
       "  'c2)C1'],\n",
       " ['CS(=O)(=O)', 'c1cn[nH]c1', '[C@H]1CC', 'N(C(=O)', 'c2cnccn2)', 'C1']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Module import RDK\n",
    "RDK.smile_tokenize(train[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O=C(', 'NC(=O)', 'c1cccc(F)c1)', 'O[C@H]1', 'CCS(=O)(=O)', 'C1'],\n",
       " ['Cn1cc',\n",
       "  'nc1C',\n",
       "  '[N@@H+]',\n",
       "  '1',\n",
       "  'C[C@@H](O)',\n",
       "  '[C@H](O',\n",
       "  'c2cccc(',\n",
       "  'C(=O)',\n",
       "  '[O-])',\n",
       "  'c2)C1'],\n",
       " ['Cn1cc',\n",
       "  'nc1C',\n",
       "  '[N@H+]',\n",
       "  '1',\n",
       "  'C[C@@H](O)',\n",
       "  '[C@H](O',\n",
       "  'c2cccc(',\n",
       "  'C(=O)',\n",
       "  '[O-])',\n",
       "  'c2)C1'],\n",
       " ['CS(=O)(=O)',\n",
       "  'c1cn',\n",
       "  '[nH]c1',\n",
       "  '[C@H]1',\n",
       "  'CCN(C(=O)',\n",
       "  'c2cn',\n",
       "  'cc',\n",
       "  'n2)C1']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Module import RDK\n",
    "RDK.smile_tokenize(train[1:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "def Chem_generator(smiles):\n",
    "    res_list = []\n",
    "    for i in tqdm(smiles):\n",
    "        mol = Chem.MolFromSmiles(i)\n",
    "        temp = []\n",
    "        index = 0\n",
    "        while(len(set(temp))!=4 and index != 100):\n",
    "            index+=1\n",
    "            temp.append(Chem.MolToSmiles(mol,doRandom=True))\n",
    "        res_list+=list(set(temp))\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1849/12525050 [00:01<1:56:13, 1795.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12525050/12525050 [1:25:08<00:00, 2451.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train_1 = Chem_generator(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "def scaled_dot_product(Q,K,V):\n",
    "        mat_QK = tf.matmul(Q,K,transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        logits = mat_QK / tf.math.sqrt(depth)\n",
    "    \n",
    "\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "class self_multihead(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,name='multiheadatten'):\n",
    "        super(self_multihead,self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model%num_heads == 0\n",
    "\n",
    "        self.depth = d_model//self.num_heads\n",
    "        self.Q = Dense(units=d_model)\n",
    "        self.K = Dense(units=d_model)\n",
    "        self.V = Dense(units=d_model)\n",
    "\n",
    "        self.linear = Dense(units = d_model)\n",
    "\n",
    "    def split(self,inputs,batch_size):\n",
    "        inputs = tf.reshape(inputs, [batch_size,-1,self.num_heads,self.depth])\n",
    "        return tf.transpose(inputs,perm=[0,2,1,3])\n",
    "\n",
    "    def call(self,inputs):\n",
    "        Q,K,V = inputs,inputs,inputs\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        \n",
    "        # Denselayer\n",
    "        Q = self.Q(Q)\n",
    "        K = self.K(K)\n",
    "        V = self.V(V)\n",
    "        #Splithead \n",
    "\n",
    "        Q = self.split(Q,batch_size)\n",
    "        K = self.split(K,batch_size)\n",
    "        V = self.split(V,batch_size)\n",
    "        #Scaled dot product\n",
    "        scaled_att,_ = scaled_dot_product(Q,K,V)\n",
    "        #(batchsize,sentence_size,num_heads,word_size/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_att, perm=[0,2,1,3])\n",
    "        #Concate\n",
    "        concat_attention = tf.reshape(scaled_attention, [batch_size, -1, self.d_model])\n",
    "        #Linear Dense\n",
    "        outputs = self.linear(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def scheduler(epochs,lr):\n",
    "    lr = 0.5*(1+np.cos(np.pi * epochs / 100))\n",
    "    if epochs <= 5:\n",
    "        lr *= epochs * 1.0 / 30\n",
    "    return lr\n",
    "\n",
    "class transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,ff_dim,name):\n",
    "        super(transformer,self).__init__(name=name)\n",
    "        self.attn = self_multihead(d_model,num_heads)\n",
    "        self.ff = Sequential([Dense(ff_dim,activation='relu'),Dense(d_model)])\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        attn_out = self.attn(inputs)\n",
    "        out1 = self.norm1(inputs + attn_out)\n",
    "        ffn_output = self.ff(out1)\n",
    "        return self.norm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,mask_zero = True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        mask = self.token_emb.compute_mask(x)\n",
    "        return x + positions\n",
    "    \n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        l2_reg = tf.keras.regularizers.l2(0.01)\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim,kernel_regularizer=l2_reg,dropout=0.1)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_zeros_with_ones(input_tensor):\n",
    "    # input_tensor에서 0인 값을 1로 바꾸는 연산\n",
    "    modified_tensor = tf.where(tf.equal(input_tensor, 0), 1, input_tensor)\n",
    "    return modified_tensor\n",
    "\n",
    "\n",
    "def calculate_means(input_list, index_ranges):\n",
    "    # TensorFlow 상수로 입력 리스트를 변환\n",
    "    input_tensor = tf.constant(input_list)\n",
    "    \n",
    "    # 각각의 인덱스 범위를 나누어 평균 계산\n",
    "    averages = [tf.reduce_mean(input_tensor[start:end]) for start, end in index_ranges]\n",
    "    \n",
    "    return averages\n",
    "\n",
    "\n",
    "class Mask(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Mask, self).__init__()\n",
    "    def call(self, inputs,mask):\n",
    "        prob = np.random.rand(1)[0]\n",
    "        if prob<0.8:\n",
    "            return tf.multiply(inputs,((mask-1)*(-1)))\n",
    "        elif prob>=0.8:\n",
    "            random_value = tf.random.uniform(shape=[inputs.shape[0]], minval=0, maxval=inputs.shape[1], dtype=tf.int32)\n",
    "            random_value = mask*random_value[:,tf.newaxis]\n",
    "            random_value = replace_zeros_with_ones(random_value)\n",
    "            return tf.multiply(inputs,(random_value))\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        #self.encoder = tf.keras.Sequential([TransformerBlock(emb_dim,num_heads,ff_dim) for i in range(8)])\n",
    "        self.encoder = TransformerBlock(emb_dim,num_heads,ff_dim)\n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = layers.Dense(len(molecule_dictionary),activation = 'softmax')\n",
    "    def call(self, inputs, mask_index,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "            \n",
    "        hidden = self.embedding(inputs)\n",
    "        \n",
    "        for i in range(8):\n",
    "            hidden = self.encoder(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 20:10:22.768775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 360 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=2000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = float(step+1)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(1e-4) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(256,10000)\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(temp_learning_rate_schedule, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.single_pred import Tox\n",
    "from Module import RDK as rk\n",
    "import atomInSmiles\n",
    "from Module import Fine_tune\n",
    "from SmilesPE import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, Value, Array\n",
    "from multiprocessing import Process,Manager,current_process\n",
    "from Module import RDK as rk\n",
    "import atomInSmiles \n",
    "from SmilesPE import tokenizer\n",
    "train_set = RDK.smile_tokenize(train_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "length_list = {}\n",
    "\n",
    "\n",
    "for index,i in enumerate(train_set):\n",
    "    try:\n",
    "        length_list[len(i)]=length_list[len(i)]+1\n",
    "    except:\n",
    "        length_list[len(i)] = 1\n",
    "    \n",
    "plt.bar(length_list.keys(),length_list.values())\n",
    "plt.xlabel(xlabel='Token_size')\n",
    "plt.ylabel('counts')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P@H+] 48\n",
      "[P@@H+] 48\n",
      "9 12\n",
      "%10 4\n",
      "[CH] 16\n",
      "p 16\n",
      "[CH2] 24\n",
      "[Si-] 4\n",
      "[P@+] 33\n",
      "[P@@+] 31\n",
      "[NH] 8\n",
      "[17O] 12\n",
      "[p+] 4\n"
     ]
    }
   ],
   "source": [
    "temp_dict = {}\n",
    "for i in train_set:\n",
    "    for j in i:\n",
    "        try:\n",
    "            temp_dict[j] = temp_dict[j] + 1\n",
    "        except:\n",
    "            temp_dict[j] = 1\n",
    "remove_dict = {}\n",
    "for i in temp_dict.keys():\n",
    "    if temp_dict[i]<50:\n",
    "        print(i,temp_dict[i])\n",
    "        remove_dict[i] = 1\n",
    "\n",
    "remove_list = []\n",
    "for index,i in enumerate(train_set):\n",
    "    for j in i:\n",
    "        try:\n",
    "            remove_dict[j]\n",
    "            remove_list.append(index)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "remove_list.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50099943\n"
     ]
    }
   ],
   "source": [
    "for i in remove_list:\n",
    "    train_set.pop(i)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./Pretraning_data/Random_ZINC_50M_SmilesPE_tokken.pkl','wb') as file:\n",
    "    pickle.dump(train_set,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./Pretraning_data/Random_ZINC_50M_SmilesPE_tokken.pkl','rb') as file:\n",
    "    train_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3030"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "molecule_dictionary = {'<start>':1,'<end>':2,'<unknown1>':3,'<unknown2>':4,'<unknown3>':5,'<unknow4>':6,'<unknown5>':7}\n",
    "\n",
    "for molecule in train_set:\n",
    "    for atom in molecule:\n",
    "        try:\n",
    "            molecule_dictionary[atom]\n",
    "        except:\n",
    "            molecule_dictionary[atom] = len(molecule_dictionary)+1\n",
    "with open('./BERT/SmiletoPE/1M_random_ZINC_word2index.pkl','wb') as file:\n",
    "    pickle.dump(molecule_dictionary,file)\n",
    "len(molecule_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50099508 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50099508/50099508 [15:09<00:00, 55106.81it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "\n",
    "def word_to_index(train_set,dict):\n",
    "    result = []\n",
    "    for molecule in tqdm(train_set):\n",
    "        temp_list = []\n",
    "        temp_list.append(1)\n",
    "        for atom in molecule:\n",
    "            temp_list.append(dict[atom])\n",
    "        while len(temp_list)!=200:\n",
    "            temp_list.append(0)\n",
    "        result.append(temp_list)\n",
    "    return result\n",
    "embedding_word = word_to_index(train_set,molecule_dictionary)   \n",
    "embedding_word = np.array(embedding_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3405.05it/s]\n",
      "100%|██████████| 18846/18846 [00:00<00:00, 70101.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tdc.single_pred import Tox\n",
    "from Module import RDK\n",
    "import atomInSmiles\n",
    "\n",
    "def word_to_index(train_set,dict):\n",
    "    result = []\n",
    "    for molecule in tqdm(train_set):\n",
    "        temp_list = []\n",
    "        temp_list.append(1)\n",
    "        for atom in molecule:\n",
    "            temp_list.append(dict[atom])\n",
    "        while len(temp_list)!=200:\n",
    "            temp_list.append(0)\n",
    "        result.append(temp_list)\n",
    "    return result\n",
    "train_val,tox_info = Tox(name = 'AMES').get_data(format = 'DeepPurpose')\n",
    "train_set_val = []\n",
    "\n",
    "train_val = Chem_generator(train_val[:5000])\n",
    "\n",
    "train_set_val = RDK.smile_tokenize(train_val)\n",
    "val_remove_list = []\n",
    "for index,i in enumerate(train_set_val):\n",
    "    if len(i) < 3:\n",
    "        val_remove_list.append(index)\n",
    "        continue\n",
    "    for j in i:\n",
    "        try:\n",
    "            molecule_dictionary[j]\n",
    "        except:\n",
    "            val_remove_list.append(index)\n",
    "            break\n",
    "val_remove_list.sort(reverse=True)\n",
    "\n",
    "for i in val_remove_list:\n",
    "    train_set_val.pop(i)\n",
    "\n",
    "embedding_word_val = word_to_index(train_set_val,molecule_dictionary)   \n",
    "embedding_word_val = np.array(embedding_word_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  315,  316, ...,    0,    0,    0],\n",
       "       [   1,  923,  202, ...,    0,    0,    0],\n",
       "       [   1,  315,  353, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,  990,   13, ...,    0,    0,    0],\n",
       "       [   1,   59, 1033, ...,    0,    0,    0],\n",
       "       [   1, 1325, 1721, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_word_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:00<00:00, 241074.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "max = 16\n",
    "mask_input_val = []\n",
    "for j in tqdm(train_set_val):\n",
    "        value = []\n",
    "        number = int(len(j)*0.15)\n",
    "        if number>max:\n",
    "                max = number\n",
    "        if number == 0:\n",
    "                number = 1\n",
    "        value += random.sample(range(1,len(j)),number)\n",
    "        mask_input_val.append(value)\n",
    "        \n",
    "        \n",
    "for j in mask_input_val:\n",
    "        while(len(j)<max):\n",
    "                j.append(-1)\n",
    "\n",
    "\n",
    "random_value_val = embedding_word_val.copy()        \n",
    "for _,index in enumerate(mask_input_val):\n",
    "        for j in index:\n",
    "                if j != -1:\n",
    "                        prob = np.random.rand(1)[0]\n",
    "                        if prob < 0.8:\n",
    "                                random_value_val[_][j] = 0\n",
    "                        elif prob > 0.9:\n",
    "                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                random_value_val[_][j] = temp1\n",
    "                                \n",
    "output_val = tf.multiply(tf.reduce_sum(tf.one_hot(mask_input_val,200),axis=1),embedding_word_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import *\n",
    "early_stopping_cb = EarlyStopping(patience=6, monitor='loss',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    output = tf.one_hot(tf.cast(tf.boolean_mask(y_true,tf.cast(y_true,bool)),tf.int32),len(molecule_dictionary))\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()(output,y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_metric(y_true,y_pred):\n",
    "    predictions = tf.argmax(y_pred,axis=1)\n",
    "    true = tf.boolean_mask(y_true,tf.cast(y_true,bool))\n",
    "    return  tf.metrics.Accuracy()(predictions,true)\n",
    "def Mask_acc(y_true, y_pred):\n",
    "    score = tf.py_function(func=Custom_metric, inp=[y_true, y_pred], Tout=tf.float32,  name='Custom_acc') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        self.transformer = TransformerBlock(emb_dim,num_heads,ff_dim)\n",
    "        \n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = layers.Dense(len(molecule_dictionary),activation = 'softmax')\n",
    "    def call(self, inputs, mask_index,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "        \n",
    "        hidden = self.embedding(inputs)\n",
    "        for i in range(8):\n",
    "            hidden = self.transformer(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,mask_zero = True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " bert_3 (BERT)                  (None, 3030)         3876300     ['input_7[0][0]',                \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,876,300\n",
      "Trainable params: 3,876,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "mask = Input(shape = (16), dtype=tf.int32)\n",
    "outputs = BERT(256,6,1024)(inputs,mask,pretrain=True)\n",
    "\n",
    "model = Model(inputs = [inputs,mask], outputs = [outputs])\n",
    "model.summary()\n",
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "model.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=2000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.batch_count\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    self.batch_count += 1\n",
    "    step = float(step+1)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(1000.) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps=4000):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        self.step.assign_add(1)  # 각 배치가 지나갈 때마다 step 증가\n",
    "        lr = tf.cond(\n",
    "            self.step <= self.warmup_steps,\n",
    "            lambda: (1e-4 - 1e-9) / self.warmup_steps * tf.cast(self.step, tf.float32) + 1e-9,\n",
    "            lambda: 0.5 * (1e-4 + 1e-9) * (self.warmup_steps ** 0.5) / (tf.cast(self.step, tf.float32) ** 0.5)\n",
    "        )\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = CustomLearningRateScheduler()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./BERT/SmiletoPE/Random_ZINC_L_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 27748/300000 [00:00<00:00, 277461.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 358497.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 50 number step\n",
      "1172/1172 [==============================] - 366s 306ms/step - loss: 5.6862 - Mask_acc: 0.1199\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.2875e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 358751.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 51 number step\n",
      "1172/1172 [==============================] - 367s 307ms/step - loss: 5.7314 - Mask_acc: 0.1203\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.41125e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 357069.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 52 number step\n",
      "1172/1172 [==============================] - 367s 307ms/step - loss: 5.6373 - Mask_acc: 0.1206\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.535e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 360532.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 53 number step\n",
      "1172/1172 [==============================] - 367s 307ms/step - loss: 5.6283 - Mask_acc: 0.1181\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.65875e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 353718.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 54 number step\n",
      "1172/1172 [==============================] - 367s 308ms/step - loss: 5.6377 - Mask_acc: 0.1190\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.7825e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 357588.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 55 number step\n",
      "1172/1172 [==============================] - 367s 307ms/step - loss: 5.7251 - Mask_acc: 0.1218\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.90625e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 358031.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 56 number step\n",
      "1172/1172 [==============================] - 367s 308ms/step - loss: 5.6382 - Mask_acc: 0.1211\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.03e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 349715.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 57 number step\n",
      "1172/1172 [==============================] - 367s 308ms/step - loss: 5.6833 - Mask_acc: 0.1236\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.15375e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 353047.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 58 number step\n",
      "1172/1172 [==============================] - 365s 306ms/step - loss: 5.6257 - Mask_acc: 0.1225\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.2775e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 352017.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 59 number step\n",
      "1172/1172 [==============================] - 366s 307ms/step - loss: 5.6713 - Mask_acc: 0.1266\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.40125e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 346582.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 60 number step\n",
      "1172/1172 [==============================] - 376s 315ms/step - loss: 5.5470 - Mask_acc: 0.1229 - val_loss: 6.2879 - val_Mask_acc: 0.1089\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.525e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 351267.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 61 number step\n",
      "1172/1172 [==============================] - 367s 308ms/step - loss: 5.5774 - Mask_acc: 0.1203\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.64875e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 335732.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 62 number step\n",
      "1172/1172 [==============================] - 367s 308ms/step - loss: 5.6063 - Mask_acc: 0.1261\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.7725e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 355352.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 63 number step\n",
      "1172/1172 [==============================] - 411s 308ms/step - loss: 5.6631 - Mask_acc: 0.1233\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.89625e-05>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 323185.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 64 number step\n",
      " 379/1172 [========>.....................] - ETA: 4:03 - loss: 5.6249 - Mask_acc: 0.1247"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "size = 300000\n",
    "for k in range(1):\n",
    "        for i in range(0,int(len(embedding_word)/size)):\n",
    "                if k == 0:\n",
    "                        i = i + 50\n",
    "                if i == int(len(embedding_word)/size):\n",
    "                        break\n",
    "                if i <80:\n",
    "                        lr = (1e-4-1e-6)*(i)/80 + 1e-6\n",
    "                else:\n",
    "                        lr = 1e-4*np.sqrt(80)/np.sqrt(i)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)\n",
    "                max = 16\n",
    "                mask_input = []\n",
    "                for j in tqdm(train_set[size*i:size*(i+1)]):\n",
    "                        value = []\n",
    "                        number = int(len(j)*0.15)\n",
    "                        if number>max:\n",
    "                                number = max\n",
    "                        if number == 0:\n",
    "                                number = 1\n",
    "                        value += random.sample(range(1,len(j)),number)\n",
    "                        mask_input.append(value)\n",
    "                        \n",
    "                        \n",
    "                for j in mask_input:\n",
    "                        while(len(j)<max):\n",
    "                                j.append(-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                inputs1 = embedding_word[size*i:size*(i+1)]\n",
    "                inputs2 = mask_input\n",
    "                output = tf.multiply(tf.reduce_sum(tf.one_hot(inputs2,200),axis=1),inputs1)\n",
    "\n",
    "                \n",
    "                random_value = inputs1.copy()        \n",
    "                for _,index in enumerate(inputs2):\n",
    "                        for j in index:\n",
    "                                if j != -1:\n",
    "                                        prob = np.random.rand(1)[0]\n",
    "                                        if prob < 0.8:\n",
    "                                                random_value[_][j] = 0\n",
    "                                        elif prob > 0.9:\n",
    "                                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                                random_value[_][j] = temp1\n",
    "                                                \n",
    "                \n",
    "                \n",
    "                print(f'This is {i} number step')\n",
    "                with tf.device('/device:GPU:0'):\n",
    "                        if i % 20 == 0:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=256,callbacks = [early_stopping_cb],validation_data=([np.array(random_value_val),np.array(mask_input_val)],np.array(output_val)))\n",
    "                        else:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=256,callbacks = [early_stopping_cb])\n",
    "                        print(model.optimizer.lr)\n",
    "                        model.save_weights('./BERT/SmiletoPE/Random_ZINC_L_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weights = model.get_weights()\n",
    "all_weights = temp_weights[:16]*8 + temp_weights[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([TransformerBlock(emb_dim,num_heads,ff_dim) for i in range(8)])\n",
    "        #self.encoder = TransformerBlock(emb_dim,num_heads,ff_dim)\n",
    "        #self.normalize = tf.keras.layers.LayerNormalization(epsilon=1e-8)\n",
    "        \n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = layers.Dense(len(molecule_dictionary),activation = 'softmax')\n",
    "    def call(self, inputs, mask_index,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "            \n",
    "        hidden = self.embedding(inputs)\n",
    "        hidden = self.encoder(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " bert_1 (BERT)                  (None, 3030)         18606540    ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,606,540\n",
      "Trainable params: 18,606,540\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "mask = Input(shape = (16), dtype=tf.int32)\n",
    "outputs = BERT(256,6,1024)(inputs,mask,pretrain=True)\n",
    "\n",
    "model = Model(inputs = [inputs,mask], outputs = [outputs])\n",
    "model.summary()\n",
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "model.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)\n",
    "model.set_weights(all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_value_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 65271/300000 [00:00<00:00, 332004.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:00<00:00, 354049.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 0 number step\n",
      " 270/1172 [=====>........................] - ETA: 4:52 - loss: 5.7561 - Mask_acc: 0.1201"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/device:GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m                 \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_value_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_input_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m                 model\u001b[38;5;241m.\u001b[39mfit([np\u001b[38;5;241m.\u001b[39marray(random_value),np\u001b[38;5;241m.\u001b[39marray(inputs2)],np\u001b[38;5;241m.\u001b[39marray(output,dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m),epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,callbacks \u001b[38;5;241m=\u001b[39m [early_stopping_cb])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "size = 300000\n",
    "for k in range(11):\n",
    "        for i in range(0,int(len(embedding_word)/size)):\n",
    "                if k == 1:\n",
    "                        i = i\n",
    "                if i == int(len(embedding_word)/size):\n",
    "                        break\n",
    "                if i <80:\n",
    "                        lr = (1e-4-1e-6)*(i)/80 + 1e-6\n",
    "                else:\n",
    "                        lr = 1e-4*np.sqrt(80)/np.sqrt(i)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "                model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)\n",
    "                max = 16\n",
    "                mask_input = []\n",
    "                for j in tqdm(train_set[size*i:size*(i+1)]):\n",
    "                        value = []\n",
    "                        number = int(len(j)*0.15)\n",
    "                        if number>max:\n",
    "                                number = max\n",
    "                        if number == 0:\n",
    "                                number = 1\n",
    "                        value += random.sample(range(1,len(j)),number)\n",
    "                        mask_input.append(value)\n",
    "                        \n",
    "                        \n",
    "                for j in mask_input:\n",
    "                        while(len(j)<max):\n",
    "                                j.append(-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                inputs1 = embedding_word[size*i:size*(i+1)]\n",
    "                inputs2 = mask_input\n",
    "                output = tf.multiply(tf.reduce_sum(tf.one_hot(inputs2,200),axis=1),inputs1)\n",
    "\n",
    "                \n",
    "                random_value = inputs1.copy()        \n",
    "                for _,index in enumerate(inputs2):\n",
    "                        for j in index:\n",
    "                                if j != -1:\n",
    "                                        prob = np.random.rand(1)[0]\n",
    "                                        if prob < 0.8:\n",
    "                                                random_value[_][j] = 0\n",
    "                                        elif prob > 0.9:\n",
    "                                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                                random_value[_][j] = temp1\n",
    "                                                \n",
    "                \n",
    "                \n",
    "                print(f'This is {i} number step')\n",
    "                with tf.device('/device:GPU:0'):\n",
    "                        if i % 20 == 0:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=256,callbacks = [early_stopping_cb],validation_data=([np.array(random_value_val),np.array(mask_input_val)],np.array(output_val)))\n",
    "                        else:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=256,callbacks = [early_stopping_cb])\n",
    "                        print(model.optimizer.lr)\n",
    "                        model.save_weights('./BERT/SMILE/Random_ZINC_L_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_weights = model.get_weights()\n",
    "res_weights = res_weights[:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = BERT(256,6,1024)(inputs,None)\n",
    "\n",
    "res_model = Model(inputs = [inputs,mask], outputs = [outputs])\n",
    "res_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model.save_weights('./BERT/SmiletoPE/Pre_BERT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
