{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 09:32:23.488460: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 09:32:23.648011: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-28 09:32:24.551421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2023-12-28 09:32:24.551520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2023-12-28 09:32:24.551527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-12-28 09:32:26.750914: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 09:32:27.549234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78900 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import execute_tensor as execute\n",
    "from execute_tensor import execute as exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module import custom_layers\n",
    "import tensorflow as tf\n",
    "\n",
    "BERT_tensor = custom_layers.BERT_tensor\n",
    "\n",
    "bert_layer = BERT_tensor(256,6,1024,strat_index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "inputs = tf.keras.layers.Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = bert_layer(inputs,None)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./BERT/atomInSmile/Pre_BERT')\n",
    "paras = model.get_weights()\n",
    "bert_layer.set_weights(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Task_mask(num_task):\n",
    "    result = np.zeros([200,200])\n",
    "    for i in range(num_task):\n",
    "        for j in range(200):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                result[j][i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Task_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.metrics import AUC\n",
    "Models = []\n",
    "\n",
    "Input = tf.keras.layers.Input(200,)\n",
    "hidden = bert_layer(Input,att_mask = mask)\n",
    "output = hidden[:,0]\n",
    "output = tf.keras.layers.Dense(256,activation = 'gelu')(output)\n",
    "output = tf.keras.layers.Dense(1)(output)\n",
    "globals()[f'Task{0}_model'] = Model(inputs = [Input],outputs = [output])\n",
    "globals()[f'Task{0}_model'].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
    "Models.append(globals()[f'Task{0}_model'])\n",
    "\n",
    "for i in range(9):\n",
    "    i = i+1\n",
    "    Input = tf.keras.layers.Input(200,)\n",
    "    hidden = bert_layer(Input,att_mask = mask)\n",
    "    output = hidden[:,i]\n",
    "    output = tf.keras.layers.Dense(256,activation = 'gelu')(output)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(output)\n",
    "    globals()[f'Task{i}_model'] = Model(inputs = [Input],outputs = [output])\n",
    "    globals()[f'Task{i}_model'].compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "    Models.append(globals()[f'Task{i}_model'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tdc.single_pred import Tox\n",
    "from Module import RDK\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,mask_zero = True)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = np.array([0]+[0]+[i+2 for i in range(198)])\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions    \n",
    "    \n",
    "    \n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        l2_reg = tf.keras.regularizers.l2(0.01)\n",
    "        \n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim,kernel_regularizer=l2_reg)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs,attention_mask=tf.cast(np.array([[1]+[0]+[1]*198] + [[0]+[1]*199] + [[0]*2+[1]*198]*198),bool))\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([TransformerBlock(emb_dim,num_heads,ff_dim) for i in range(8)])\n",
    "        \n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = tf.keras.layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = tf.keras.layers.Dense(71,activation = 'softmax')\n",
    "    def call(self, inputs, mask_index=None,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "        \n",
    "        inputs = tf.reshape(inputs,[-1,200])\n",
    "        hidden = self.embedding(inputs)\n",
    "        \n",
    "        \n",
    "        hidden = self.encoder(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden\n",
    "\n",
    "\n",
    "def predict(model,results,len_list):\n",
    "    index = 0\n",
    "    res = model.predict(results,verbose=0)\n",
    "    x_val = []\n",
    "    for i in len_list:\n",
    "        temp = res[index:index+i]\n",
    "        x_val.append(np.average(temp,axis=0))\n",
    "        index = index+i\n",
    "    return np.array(x_val)\n",
    "\n",
    "def similar(a, b):    return SequenceMatcher(None, a, b).ratio()\n",
    "def most_similar(query,word2idx):\n",
    "\n",
    "    max = 0\n",
    "    tokken = ''\n",
    "    query = query.split(';')\n",
    "    for i in word2idx.keys():\n",
    "        key = i.split(';')\n",
    "        temp2 = 0\n",
    "        temp3 = 0\n",
    "        temp1 = similar(query[0],key[0])*10\n",
    "        try:\n",
    "            temp2 = similar(query[1],key[1])*2\n",
    "            temp3 = similar(query[2],key[2])*1\n",
    "        except:\n",
    "            pass\n",
    "        temp = temp1+temp2+temp3\n",
    "        if temp>max:\n",
    "            max = temp\n",
    "            tokken = i\n",
    "    return tokken\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_val,y_true,len_20):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        index = 0\n",
    "        res = []\n",
    "        for i in len_20:\n",
    "            res.append(np.average(y_true[index:index+i]))\n",
    "            index = index+i\n",
    "        self.len20 = len_20\n",
    "        self.counts = []\n",
    "        self.max = 0\n",
    "        self.y_true = np.array(res)\n",
    "        self.history = {}\n",
    "        self.epoch = 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 에포크가 끝날 때마다 validation 데이터로 모델 평가\n",
    "        result = predict(self.model,self.x_val,self.len20)\n",
    "        acc = Accuracy()(self.y_true,np.round(result))\n",
    "        auc_res = (AUC()(self.y_true,result)).numpy()\n",
    "        loss = tf.keras.metrics.BinaryCrossentropy()(self.y_true,result)\n",
    "        auc_res = auc_res\n",
    "        print(f\"     val_acc : {acc},    val_auc : {auc_res} val_loss : {loss}\")\n",
    "        if 'val_acc' not in self.history:\n",
    "            self.history['val_acc'] = [acc]\n",
    "        else:\n",
    "            self.history['val_acc'] += [acc]\n",
    "        if 'val_auc' not in self.history:\n",
    "            self.history['val_auc'] = [auc_res]\n",
    "        else:\n",
    "            self.history['val_auc'] += [auc_res]\n",
    "        \n",
    "        if 'val_loss' not in self.history:\n",
    "            self.history['val_loss'] = [loss]\n",
    "        else:\n",
    "            self.history['val_loss'] += [loss]\n",
    "        \n",
    "        self.max = np.max(self.history['val_auc'])\n",
    "            \n",
    "        if self.history['val_auc'][-1]<self.max:\n",
    "            self.counts.append(1)\n",
    "        else:\n",
    "            self.counts = []\n",
    "\n",
    "        self.epoch += 1\n",
    "        \"\"\"\n",
    "        if self.epoch>10 and len(self.counts)>2:\n",
    "            self.model.stop_training = True\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = BERT(256,6,1024)(inputs,None)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "#model.load_weights('./BERT/atomInSmile/F_Random_ZINC_L_model_weights.h5')\n",
    "\n",
    "\n",
    "#BERT_parameters = model.get_weights()[:130]\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def BERT_model():\n",
    "    inputs = Input(200,)\n",
    "    hidden = bert_layer(inputs)\n",
    "    hidden = hidden[:,0]\n",
    "    hidden = tf.keras.layers.Dense(256,activation = 'gelu')(hidden)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(hidden)\n",
    "    result = Model(inputs = [inputs],outputs = [output])\n",
    "    return result\n",
    "\n",
    "def Bit_model():\n",
    "    inputs = Input(2048,)\n",
    "    hidden = tf.keras.layers.Dense(250,activation = 'relu')(inputs)\n",
    "    hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    hidden = tf.keras.layers.Dense(40,activation = 'relu')(hidden)\n",
    "    hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    hidden = tf.keras.layers.Dense(10,activation = 'relu')(hidden)\n",
    "    hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(hidden)\n",
    "    result = Model(inputs = [inputs],outputs = [output])\n",
    "    return result\n",
    "\n",
    "\n",
    "    \n",
    "class tox_process():\n",
    "    def __init__(self,tox,test_size=0.2,random_state = 1024):\n",
    "        self.tox_name = tox\n",
    "        self.size = test_size\n",
    "        self.seed = random_state\n",
    "        \n",
    "        \n",
    "    def AIS_process(self,plot=False,token = 'AIS',number_of_task = 2):\n",
    "        if token == 'AIS':\n",
    "            with open('./Tox_data/AIS_Tox_data/'+self.tox_name,'rb') as file:\n",
    "                train,label,len_20 = pickle.load(file)[0]\n",
    "            with open('./BERT/atomInSmile/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "                word2idx = pickle.load(file)\n",
    "        elif token == 'SMILE':\n",
    "            with open('./Tox_data/SMILE_Tox_data/'+self.tox_name,'rb') as file:\n",
    "                train,label,len_20 = pickle.load(file)[0]\n",
    "            with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "                word2idx = pickle.load(file)\n",
    "                \n",
    "            \"\"\"elif token == 'SmiletoPE':\n",
    "                with open('./Tox_data/SmiletoPE/'+self.tox_name,'rb') as file:\n",
    "                    train,label,len_20 = pickle.load(file)[0]\n",
    "                with open('./BERT/SmiletoPE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "                    word2idx = pickle.load(file)\"\"\"\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            temp_dict = {}\n",
    "\n",
    "            for i in train:\n",
    "                try:\n",
    "                    temp_dict[len(i)] = temp_dict[len(i)] + 1\n",
    "                except:\n",
    "                    temp_dict[len(i)] = 1\n",
    "                    \n",
    "            plt.bar(temp_dict.keys(),temp_dict.values())\n",
    "        \n",
    "        except_dict = {}\n",
    "\n",
    "        for i in train:\n",
    "            for j in i:\n",
    "                try:\n",
    "                    word2idx[j]\n",
    "                except:\n",
    "                    try:\n",
    "                        except_dict[j]\n",
    "                    except:\n",
    "                        except_dict[j] = len(except_dict) + 1\n",
    "        \n",
    "        similar_dict = {}\n",
    "\n",
    "        for i in except_dict.keys():\n",
    "            similar_dict[i] = most_similar(i,word2idx)\n",
    "            \n",
    "            \n",
    "        AIS_train = []\n",
    "        for index,i in enumerate(train):\n",
    "            \n",
    "            temp = []\n",
    "            if number_of_task == 2:\n",
    "                temp.append(3)\n",
    "                temp.append(4)\n",
    "            else:\n",
    "                for k in range(number_of_task):\n",
    "                    temp.append(len(word2idx)+1+k)\n",
    "            temp.append(1)\n",
    "            for j in i:\n",
    "                try:\n",
    "                    temp.append(word2idx[j])\n",
    "                except:\n",
    "                    print('Unexpected : ',j)\n",
    "                    word2idx[j] = len(word2idx)+1\n",
    "                    temp.append(word2idx[j])\n",
    "                \"\"\"    \n",
    "                except:\n",
    "                    if j != '/[H]':\n",
    "                        print(j,i)\n",
    "                        word_sim = similar_dict[j]\n",
    "                        if word_sim != '':\n",
    "                            temp.append(word2idx[word_sim])\n",
    "                    else:\n",
    "                        pass\"\"\"\n",
    "            if len(temp)>1:\n",
    "                AIS_train.append(temp)\n",
    "\n",
    "        AIS_train = tf.keras.preprocessing.sequence.pad_sequences(AIS_train, padding='post', maxlen=200)\n",
    "        temp_x = []\n",
    "        temp_y = []\n",
    "        index = 0\n",
    "        for i in len_20:\n",
    "            temp_x.append(AIS_train[index:index+i])\n",
    "            temp_y.append(label[index:index+i])\n",
    "            index = index+i\n",
    "\n",
    "\n",
    "\n",
    "        x_train, x_val, y_train, y_val,_,len_20 = train_test_split(temp_x,temp_y,len_20, test_size=self.size,random_state=self.seed)\n",
    "        \n",
    "        def flatten(data):\n",
    "            temp = []\n",
    "            for i in data:\n",
    "                temp+=list(i)\n",
    "            data = np.array(temp)\n",
    "            return data\n",
    "        x_train = flatten(x_train)\n",
    "        x_val = flatten(x_val)\n",
    "        y_val = flatten(y_val)\n",
    "        y_train = flatten(y_train)\n",
    "        return x_train, x_val, y_train, y_val,len_20\n",
    "    \n",
    "    def bit_precess(self):\n",
    "        train,tox_info = Tox(name=self.tox_name).get_data(format='DeepPurpose')\n",
    "        bit_string = RDK.smile_to_RDkit(train,2048)\n",
    "        x_train_NN,x_val_NN,y_train_NN,y_val_NN = train_test_split(np.array(bit_string)/1.,np.array(tox_info)/1.,test_size=self.size,random_state=self.seed)\n",
    "        return x_train_NN,x_val_NN,y_train_NN,y_val_NN\n",
    "    \n",
    "\n",
    "\n",
    "class execute():\n",
    "    def __init__(self,tox,test_size,split_seed,epoch = 20,batch=32*20,tokens = ['AIS']):\n",
    "        super().__init__()\n",
    "        self.tox = tox\n",
    "        self.size = test_size\n",
    "        self.seed = split_seed\n",
    "        self.epoch = epoch\n",
    "        self.BERTs = []\n",
    "        self.batch_size = batch\n",
    "        Bit_Classifier = Bit_model()\n",
    "        self.Bit = Bit_Classifier\n",
    "        #model.load_weights('./BERT/atomInSmile/F_Random_ZINC_L_model_weights.h5')\n",
    "        self.tokens = tokens\n",
    "        self.BERT_parameters = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token == 'AIS':\n",
    "                model.load_weights('./BERT/atomInSmile/Pre_BERT.h5')\n",
    "            elif token == 'SMILE':\n",
    "                model.load_weights('./BERT/SMILE/Pre_BERT.h5')\n",
    "            elif token == 'SmiletoPE':\n",
    "                model.load_weights('./BERT/SmiletoPE/F_Random_ZINC_L_model_weights.h5')\n",
    "            else:\n",
    "                raise\n",
    "            self.BERTs.append(BERT_model())\n",
    "            self.BERT_parameters.append(model.get_weights()[:130])\n",
    "    def forward(self,set_weights=True):\n",
    "        if set_weights:\n",
    "            for index,token in enumerate(self.tokens):\n",
    "                BERT_parameter = self.BERT_parameters[index]\n",
    "                self.BERTs[index].layers[1].set_weights(BERT_parameter)\n",
    "        \n",
    "\n",
    "        for index,token in enumerate(self.tokens): \n",
    "            process = tox_process(self.tox, self.size, self.seed)\n",
    "            x_train, x_val, y_train, y_val,len_20 = process.AIS_process(token = token)\n",
    "            \n",
    "            temp_BERT = self.BERTs[index]\n",
    "            \n",
    "            val_call = CustomCallback(x_val,y_val,len_20)\n",
    "            \n",
    "            temp_BERT.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "            \n",
    "            hist1 = temp_BERT.fit(x_train,y_train,batch_size=self.batch_size,epochs=self.epoch,callbacks=[val_call])\n",
    "            \n",
    "        \n",
    "            temp_BERT.history.history['val_loss'] = val_call.history['val_loss']\n",
    "            temp_BERT.history.history['val_acc'] = val_call.history['val_acc']\n",
    "            temp_BERT.history.history['val_auc'] = val_call.history['val_auc']\n",
    "            \n",
    "        x_train_NN,x_val_NN,y_train_NN,y_val_NN = process.bit_precess()\n",
    "        self.Bit.compile(optimizer = 'Adam',loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "        hist2 = self.Bit.fit(x_train_NN,y_train_NN,batch_size=32,epochs=self.epoch,validation_data=(x_val_NN,y_val_NN))\n",
    "        \n",
    "        \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(models,tox_name,token=['AIS']):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['loss']))],model.history.history['loss'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_loss']))],val_call2.history.history['val_loss'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' train Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 2)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['acc']))],model.history.history['acc'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_acc']))],val_call2.history.history['val_acc'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' train ACC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 3)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['auc']))],model.history.history['auc'],label=models[model])\n",
    "    plt.title(tox_name + ' train AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['val_loss']))],model.history.history['val_loss'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_loss']))],val_call2.history.history['val_loss'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' val Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 5)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['val_acc']))],model.history.history['val_acc'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_acc']))],val_call2.history.history['val_acc'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' val ACC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout() \n",
    "    plt.subplot(2, 3, 6)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['val_auc']))],model.history.history['val_auc'],label=models[model])\n",
    "    plt.title(tox_name + ' val AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "    \n",
    "    plt.savefig(f'./Results/{token[0]}_{token[1]}_Tox_result/'+tox_name+'.png')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./BERT/atomInSmile/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "    word2idx = pickle.load(file)\n",
    "def w2idx(X_train,task_num = 2):\n",
    "    res = []\n",
    "    for i in X_train:\n",
    "        temp = []\n",
    "        for k in range(task_num):\n",
    "            temp.append(k+len(word2idx)+1)\n",
    "        temp.append(1)\n",
    "        for j in i:\n",
    "            temp.append(word2idx[j])\n",
    "        res.append(temp)\n",
    "    res = tf.keras.utils.pad_sequences(res,padding='post',maxlen=200)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "Tox_names = ['LD50_Zhu','AMES','ClinTox','hERG','DILI','Skin_Reaction','hERG_Karim']\n",
    "x_trains = [] \n",
    "y_trains = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "len_20s = []\n",
    "task_labels = []\n",
    "val_task = []\n",
    "for index1,i in enumerate(Tox_names):\n",
    "    try:\n",
    "        \n",
    "        A = tox_process(i)\n",
    "        x_train,x_val,y_train,y_val,len_20 = A.AIS_process(number_of_task=10,token='AIS')\n",
    "        index = 0\n",
    "        res = []\n",
    "        for i in len_20:\n",
    "            res.append(np.average(y_val[index:index+i]))\n",
    "            index = index+i\n",
    "        y_val = np.array(res)\n",
    "        \n",
    "        temp_zip = list(zip(x_train,y_train))\n",
    "        random.shuffle(temp_zip)\n",
    "        x_train,y_train = zip(*temp_zip)\n",
    "        \n",
    "        \n",
    "        temp_x_trains = x_train[:int(len(x_train)/32)*32]\n",
    "        temp_x_trains = np.reshape(temp_x_trains,[-1,32,200])\n",
    "        x_trains += list(temp_x_trains) + list(x_train[int(len(x_train)/32)*32:])\n",
    "        \n",
    "        temp_y_trains = y_train[:int(len(y_train)/32)*32]\n",
    "        temp_y_trains = np.reshape(temp_y_trains,[-1,32])\n",
    "        tmp = list(temp_y_trains) + list(y_train[int(len(y_train)/32)*32:])\n",
    "        y_trains += tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "        task_labels+=[index1]*len(tmp)\n",
    "        \n",
    "        \n",
    "        y_vals.append(y_val)\n",
    "        x_vals.append(x_val)\n",
    "        len_20s.append(len_20)\n",
    "        val_task.append(index1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "Tox_names = ['LD50_Zhu','AMES','ClinTox','hERG','DILI','Skin_Reaction','hERG_Karim']\n",
    "x_trains = [] \n",
    "y_trains = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "len_20s = []\n",
    "task_labels = []\n",
    "val_task = []\n",
    "for index1,i in enumerate(Tox_names):\n",
    "\n",
    "    with open(f'./data_sets/{i}_train','rb') as file:\n",
    "        X_train,y_train,_ = pickle.load(file)\n",
    "        x_train = w2idx(X_train)\n",
    "    with open(f'./data_sets/{i}_val','rb') as file:\n",
    "        X_val,y_val,len_20 = pickle.load(file)\n",
    "        x_val = w2idx(X_val)\n",
    "    index = 0\n",
    "    res = []\n",
    "    for i in len_20:\n",
    "        res.append(np.average(y_val[index:index+i]))\n",
    "        index = index+i\n",
    "    y_val = np.array(res)\n",
    "    temp_zip = list(zip(x_train,y_train))\n",
    "    random.shuffle(temp_zip)\n",
    "    x_train,y_train = zip(*temp_zip)\n",
    "    \n",
    "    \n",
    "    temp_x_trains = x_train[:int(len(x_train)/32)*32]\n",
    "    temp_x_trains = np.reshape(temp_x_trains,[-1,32,200])\n",
    "    x_trains += list(temp_x_trains) + list(x_train[int(len(x_train)/32)*32:])\n",
    "    \n",
    "    temp_y_trains = y_train[:int(len(y_train)/32)*32]\n",
    "    temp_y_trains = np.reshape(temp_y_trains,[-1,32])\n",
    "    tmp = list(temp_y_trains) + list(y_train[int(len(y_train)/32)*32:])\n",
    "    y_trains += tmp\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_labels+=[index1]*len(tmp)\n",
    "    \n",
    "    \n",
    "    y_vals.append(y_val)\n",
    "    x_vals.append(x_val)\n",
    "    len_20s.append(len_20)\n",
    "    val_task.append(index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1937"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "temp_zip = list(zip(x_trains,y_trains,task_labels))\n",
    "random.shuffle(temp_zip)\n",
    "\n",
    "x_trains,y_trains,task_labels = zip(*temp_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1039, 1040,    1,   10,   11,   14,   20,    9,   37,   37,   37,\n",
       "         37,   11,   19,   20,   11,   10,    9,   20,    9,   96,   20,\n",
       "         21,   42,   61,   58,   51,    9,   15,   95,   26,   63,   64,\n",
       "         63,   26,   95,   19,   37,   42,    9,   17,   18,   19,   20,\n",
       "         61,   37,   37,   37,   21,   19,   37,   37,   12,   11,   31,\n",
       "         16,    9,   17,   18,   19,   45,   15,   11,   26,   26,   15,\n",
       "          9,   45,   50,   19,   26,   26,   11,   19,   85,   14,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m loss_fn_reg \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError()\n\u001b[1;32m      9\u001b[0m acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn_reg = tf.keras.losses.MeanSquaredError()\n",
    "acc = np.array([0])\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    loss_list = [[] for i in range(len(Tox_names))]\n",
    "    acc_list = [[] for i in range(len(Tox_names))]\n",
    "    auc_list = [[] for i in range(len(Tox_names))]\n",
    "    for i in range(0, len(temp_zip)):\n",
    "        batch_images = x_trains[i]\n",
    "        batch_images = tf.reshape(batch_images,[-1,200])\n",
    "        batch_labels = y_trains[i]\n",
    "        batch_labels = tf.reshape(batch_labels,[-1])\n",
    "        task = task_labels[i]\n",
    "\n",
    "        if task != 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "                model = globals()[f'Task{task}_model']\n",
    "                logits = model(batch_images)\n",
    "                logits = tf.reshape(logits,[-1])\n",
    "                loss_value = loss_fn(batch_labels, logits)\n",
    "                acc = tf.keras.metrics.Accuracy()(np.round(logits),batch_labels)\n",
    "                auc = tf.keras.metrics.AUC()(batch_labels,logits)\n",
    "                auc = auc.numpy()\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "                loss_list[task].append(loss_value)\n",
    "                acc_list[task].append(acc)\n",
    "                auc_list[task].append(auc)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        else:\n",
    "            with tf.GradientTape() as tape:\n",
    "                model = globals()[f'Task{task}_model']\n",
    "                logits = model(batch_images)\n",
    "                logits = tf.reshape(logits,[-1])\n",
    "                loss_value = loss_fn_reg(logits,batch_labels)\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "                loss_list[task].append(loss_value)\n",
    "                acc = tf.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None)(logits,batch_labels)\n",
    "                acc_list[task].append(acc)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        for l in range(len(Tox_names)):\n",
    "            \n",
    "            temp_loss = np.average(loss_list[l][-10000:])\n",
    "            temp_acc = np.average(acc_list[l][-10000:])\n",
    "            temp_auc = np.average(auc_list[l][-10000:])\n",
    "            if l == 0:\n",
    "                text = \"\\rSteps : {} Task : {}, Loss: {:.4f}, acc : {:.4f}, auc : {:.4f}\".format(i,l,temp_loss,temp_acc,temp_auc)\n",
    "            else:\n",
    "                text += \"     Task : {}, Loss: {:.4f}, acc : {:.4f}, auc : {:.4f}\".format(l,temp_loss,temp_acc,temp_auc)\n",
    "            sys.stdout.write(text)\n",
    "            sys.stdout.flush()\n",
    "    # 각 에포크 종료 후 평가\n",
    "    for temp_index,j in enumerate(val_task):\n",
    "        j = j\n",
    "        val_res = predict(globals()[f'Task{j}_model'],x_vals[temp_index],len_20s[temp_index])\n",
    "        if j != 0:\n",
    "            acc = tf.keras.metrics.Accuracy()(y_vals[temp_index],np.round(val_res))\n",
    "            auc_res = (AUC()(y_vals[temp_index],val_res)).numpy()\n",
    "            loss = tf.keras.metrics.BinaryCrossentropy()(y_vals[temp_index],val_res)\n",
    "            \n",
    "            print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "            print(f\"Test accuracy: {acc}\")\n",
    "            print(f\"Test AUC: {auc_res}\")\n",
    "            print(f\"Test loss: {loss}\\n\")\n",
    "        else:\n",
    "            loss = tf.keras.metrics.MeanSquaredError()(y_vals[temp_index],val_res)\n",
    "            print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "            print(f\"\\nTest MSE:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task is AMES\n",
      "Test accuracy: 0.7616758346557617\n",
      "Test AUC: 0.83214271068573\n",
      "Test loss: 0.8709271550178528\n",
      "\n",
      "\n",
      "Task is ClinTox\n",
      "Test accuracy: 0.9459459185600281\n",
      "Test AUC: 0.4715302586555481\n",
      "Test loss: 0.448618084192276\n",
      "\n",
      "\n",
      "Task is hERG\n",
      "Test accuracy: 0.8396946787834167\n",
      "Test AUC: 0.8364582657814026\n",
      "Test loss: 0.5010809302330017\n",
      "\n",
      "\n",
      "Task is DILI\n",
      "Test accuracy: 0.8105263113975525\n",
      "Test AUC: 0.8699902296066284\n",
      "Test loss: 0.4933035671710968\n",
      "\n",
      "\n",
      "Task is Skin_Reaction\n",
      "Test accuracy: 0.9196277260780334\n",
      "Test AUC: 0.5183524489402771\n",
      "Test loss: 0.6686758399009705\n",
      "\n",
      "\n",
      "Task is hERG_Karim\n",
      "Test accuracy: 0.7482335567474365\n",
      "Test AUC: 0.8138135075569153\n",
      "Test loss: 1.072299599647522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j,j in enumerate(val_task):\n",
    "    if j == 0:\n",
    "        continue\n",
    "    val_res = predict(globals()[f'Task{j}_model'],x_vals[j],len_20s[j])\n",
    "    if j != 0:\n",
    "        acc = tf.keras.metrics.Accuracy()(y_vals[j],np.round(val_res))\n",
    "        auc_res = (AUC()(y_vals[j],val_res)).numpy()\n",
    "        loss = tf.keras.metrics.BinaryCrossentropy()(y_vals[j],val_res)\n",
    "        \n",
    "        print(f'\\nTask is {Tox_names[j]}')\n",
    "        print(f\"Test accuracy: {acc}\")\n",
    "        print(f\"Test AUC: {auc_res}\")\n",
    "        print(f\"Test loss: {loss}\\n\")\n",
    "    else:\n",
    "        loss = tf.keras.metrics.MeanSquaredError()(y_vals[j],val_res)\n",
    "        print(f'\\nTask is {Tox_names[j]}')\n",
    "        print(f\"\\nTest MSE:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
