{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 11:26:47.881071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-03 11:26:48.057445: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-03 11:26:48.859612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2024-01-03 11:26:48.859711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2024-01-03 11:26:48.859717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-01-03 11:26:50.862712: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-03 11:26:51.500175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78900 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import execute_tensor as execute\n",
    "from execute_tensor import execute as exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from execute import tox_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module import custom_layers\n",
    "import tensorflow as tf\n",
    "\n",
    "BERT_tensor = custom_layers.BERT_tensor_small\n",
    "\n",
    "bert_layer = BERT_tensor(256,8,1024,strat_index=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tox_process('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from execute_tensor import predict\n",
    "\n",
    "with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "    word2idx = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "inputs = tf.keras.layers.Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = bert_layer(inputs,None)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./BERT/SMILE/small_Pre_BERT.pkl','rb') as file:\n",
    "    paras = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(paras)\n",
    "paras = model.get_weights()\n",
    "bert_layer.set_weights(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Task_mask(num_task):\n",
    "    result = np.zeros([200,200])\n",
    "    for i in range(num_task):\n",
    "        for j in range(200):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                result[j][i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Task_mask(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.metrics import AUC\n",
    "Models = []\n",
    "\n",
    "Input = tf.keras.layers.Input(200,)\n",
    "hidden = bert_layer(Input,att_mask = mask)\n",
    "output = hidden[:,0]\n",
    "output = tf.keras.layers.Dropout(0.3)(output)\n",
    "output = tf.keras.layers.Dense(100,activation = 'gelu')(output)\n",
    "output = tf.keras.layers.Dense(1,activation = 'sigmoid')(output)\n",
    "globals()[f'Task{0}_model'] = Model(inputs = [Input],outputs = [output])\n",
    "globals()[f'Task{0}_model'].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "Models.append(globals()[f'Task{0}_model'])\n",
    "\n",
    "for i in range(9):\n",
    "    i = i+1\n",
    "    Input = tf.keras.layers.Input(200,)\n",
    "    hidden = bert_layer(Input,att_mask = mask)\n",
    "    output = hidden[:,i]\n",
    "    output = tf.keras.layers.Dropout(0.3)(output)\n",
    "    output = tf.keras.layers.Dense(100,activation = 'gelu')(output)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(output)\n",
    "    globals()[f'Task{i}_model'] = Model(inputs = [Input],outputs = [output])\n",
    "    globals()[f'Task{i}_model'].compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "    Models.append(globals()[f'Task{i}_model'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "    word2idx = pickle.load(file)\n",
    "def w2idx(X_train,task_num = 2):\n",
    "    res = []\n",
    "    for i in X_train:\n",
    "        temp = []\n",
    "        for k in range(task_num):\n",
    "            temp.append(k+len(word2idx)+1)\n",
    "        temp.append(1)\n",
    "        for j in i:\n",
    "            temp.append(word2idx[j])\n",
    "        res.append(temp)\n",
    "    res = tf.keras.utils.pad_sequences(res,padding='post',maxlen=200)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "Tox_names = ['AMES','ClinTox','hERG','DILI','Skin_Reaction','hERG_Karim']\n",
    "x_trains = [] \n",
    "y_trains = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "len_20s = []\n",
    "task_labels = []\n",
    "val_task = []\n",
    "for index1,i in enumerate(Tox_names):\n",
    "    try:\n",
    "        \n",
    "        A = tox_process(i)\n",
    "        x_train,x_val,y_train,y_val,len_20 = A.AIS_process(number_of_task=10,token='AIS')\n",
    "        index = 0\n",
    "        res = []\n",
    "        for i in len_20:\n",
    "            res.append(np.average(y_val[index:index+i]))\n",
    "            index = index+i\n",
    "        y_val = np.array(res)\n",
    "        \n",
    "        temp_zip = list(zip(x_train,y_train))\n",
    "        random.shuffle(temp_zip)\n",
    "        x_train,y_train = zip(*temp_zip)\n",
    "        \n",
    "        \n",
    "        temp_x_trains = x_train[:int(len(x_train)/32)*32]\n",
    "        temp_x_trains = np.reshape(temp_x_trains,[-1,32,200])\n",
    "        x_trains += list(temp_x_trains) + list(x_train[int(len(x_train)/32)*32:])\n",
    "        \n",
    "        temp_y_trains = y_train[:int(len(y_train)/32)*32]\n",
    "        temp_y_trains = np.reshape(temp_y_trains,[-1,32])\n",
    "        tmp = list(temp_y_trains) + list(y_train[int(len(y_train)/32)*32:])\n",
    "        y_trains += tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "        task_labels+=[index1]*len(tmp)\n",
    "        \n",
    "        \n",
    "        y_vals.append(y_val)\n",
    "        x_vals.append(x_val)\n",
    "        len_20s.append(len_20)\n",
    "        val_task.append(index1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 0 0 0]\n",
      "Unexpected :  [Cu+2]\n",
      "Unexpected :  [Mn+2]\n",
      "Unexpected :  [H-]\n",
      "Unexpected :  [SbH]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "Tox_names = ['AMES','ClinTox','hERG','DILI','hERG_Karim','Skin Reaction','Carcinogens_Lagunin']\n",
    "x_trains = [] \n",
    "y_trains = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "len_20s = []\n",
    "task_labels = []\n",
    "val_task = []\n",
    "for index1,i in enumerate(Tox_names):\n",
    "    with open(f'./Tox_data/SMILE_Tox_data/{i}','rb') as file:\n",
    "        corpus = pickle.load(file)\n",
    "    spliter = tox_process('temp',0.2,204)\n",
    "    x_train,x_val,y_train,y_val,len_20 = spliter.train_val_split(corpus[0],word2idx,7)\n",
    "    print(y_train)\n",
    "    \"\"\"\n",
    "    with open(f'./data_sets/{i}_train','rb') as file:\n",
    "        X_train,y_train,_ = pickle.load(file)\n",
    "        x_train = w2idx(X_train,0)\n",
    "    with open(f'./data_sets/{i}_val','rb') as file:\n",
    "        X_val,y_val,len_20 = pickle.load(file)\n",
    "        x_val = w2idx(X_val,0)\"\"\"\n",
    "    index = 0\n",
    "    res = []\n",
    "    for i in len_20:\n",
    "        res.append(np.average(y_val[index:index+i]))\n",
    "        index = index+i\n",
    "    y_val = np.array(res)\n",
    "    temp_zip = list(zip(x_train,y_train))\n",
    "    random.shuffle(temp_zip)\n",
    "    x_train,y_train = zip(*temp_zip)\n",
    "    \n",
    "    \n",
    "    temp_x_trains = x_train[:int(len(x_train)/32)*32]\n",
    "    temp_x_trains = np.reshape(temp_x_trains,[-1,32,200])\n",
    "    x_trains += list(temp_x_trains) + list(x_train[int(len(x_train)/32)*32:])\n",
    "    \n",
    "    temp_y_trains = y_train[:int(len(y_train)/32)*32]\n",
    "    temp_y_trains = np.reshape(temp_y_trains,[-1,32])\n",
    "    tmp = list(temp_y_trains) + list(y_train[int(len(y_train)/32)*32:])\n",
    "    y_trains += tmp\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_labels+=[index1]*len(tmp)\n",
    "    \n",
    "    \n",
    "    y_vals.append(y_val)\n",
    "    x_vals.append(x_val)\n",
    "    len_20s.append(len_20)\n",
    "    val_task.append(index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "temp_zip = list(zip(x_trains,y_trains,task_labels))\n",
    "random.shuffle(temp_zip)\n",
    "\n",
    "x_trains,y_trains,task_labels = zip(*temp_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 1,\n",
       " '<end>': 2,\n",
       " '<unknown1>': 3,\n",
       " '<unknown2>': 4,\n",
       " '<unknown3>': 5,\n",
       " '<unknow4>': 6,\n",
       " '<unknown5>': 7,\n",
       " 'C': 8,\n",
       " 'n': 9,\n",
       " '1': 10,\n",
       " 'c': 11,\n",
       " '(': 12,\n",
       " 'N': 13,\n",
       " '2': 14,\n",
       " '[C@H]': 15,\n",
       " '[nH]': 16,\n",
       " 'S': 17,\n",
       " ')': 18,\n",
       " '=': 19,\n",
       " 'O': 20,\n",
       " '[C@@H]': 21,\n",
       " 'F': 22,\n",
       " '[O-]': 23,\n",
       " '[N@@H+]': 24,\n",
       " '3': 25,\n",
       " '[N@H+]': 26,\n",
       " 'o': 27,\n",
       " '4': 28,\n",
       " 'Br': 29,\n",
       " '[nH+]': 30,\n",
       " '[NH+]': 31,\n",
       " '5': 32,\n",
       " '[C@]': 33,\n",
       " '[C@@]': 34,\n",
       " 's': 35,\n",
       " '-': 36,\n",
       " '[NH3+]': 37,\n",
       " '/': 38,\n",
       " '\\\\': 39,\n",
       " 'Cl': 40,\n",
       " '[N+]': 41,\n",
       " '#': 42,\n",
       " '[N-]': 43,\n",
       " '[NH2+]': 44,\n",
       " '[P@]': 45,\n",
       " '[P@@]': 46,\n",
       " '[n-]': 47,\n",
       " '[n+]': 48,\n",
       " '[N@+]': 49,\n",
       " '[N@@+]': 50,\n",
       " 'I': 51,\n",
       " 'P': 52,\n",
       " '[PH+]': 53,\n",
       " '6': 54,\n",
       " '[S@@]': 55,\n",
       " '[Si]': 56,\n",
       " '[S@]': 57,\n",
       " '[o+]': 58,\n",
       " '[S-]': 59,\n",
       " '[O+]': 60,\n",
       " '7': 61,\n",
       " '[S@@+]': 62,\n",
       " '[S@+]': 63,\n",
       " '[S+]': 64,\n",
       " '[P@H]': 65,\n",
       " '[P@@H]': 66,\n",
       " '[SH]': 67,\n",
       " '[s+]': 68,\n",
       " '[P+]': 69,\n",
       " '[PH]': 70,\n",
       " '8': 71,\n",
       " '[Hg]': 72,\n",
       " '.': 73,\n",
       " '[Na]': 74,\n",
       " '[Cl+3]': 75,\n",
       " '[cH-]': 76,\n",
       " '[CH-]': 77,\n",
       " '[CH2-]': 78,\n",
       " '[C-]': 79,\n",
       " '[Cl+2]': 80,\n",
       " '[Cl-]': 81,\n",
       " '[Na+]': 82,\n",
       " '[NH4+]': 83,\n",
       " '[Br-]': 84,\n",
       " '[Li+]': 85,\n",
       " '[Gd+3]': 86,\n",
       " 'B': 87,\n",
       " '[Pt+2]': 88,\n",
       " '[Fe+4]': 89,\n",
       " '*': 90,\n",
       " '[H]': 91,\n",
       " '[N]': 92,\n",
       " '[99Tc]': 93,\n",
       " '[Se]': 94,\n",
       " '[Fe-2]': 95,\n",
       " '[Al]': 96,\n",
       " '[Ca+2]': 97,\n",
       " '[131I]': 98,\n",
       " '[Pt]': 99,\n",
       " '[Bi]': 100,\n",
       " '[123I]': 101,\n",
       " '[Au]': 102,\n",
       " '[201Tl]': 103,\n",
       " '[Cr]': 104,\n",
       " '[Cu]': 105,\n",
       " '[Mn]': 106,\n",
       " '[Zn]': 107,\n",
       " '[As]': 108,\n",
       " '[32P]': 109,\n",
       " '[Ti]': 110,\n",
       " '[I-]': 111,\n",
       " '9': 112,\n",
       " '%10': 113,\n",
       " '%11': 114,\n",
       " '[NH-]': 115,\n",
       " '[K+]': 116,\n",
       " '[B-]': 117,\n",
       " '[Mg+2]': 118,\n",
       " '[se]': 119,\n",
       " '[2H]': 120,\n",
       " '[Ba+2]': 121,\n",
       " '[OH-]': 122,\n",
       " '[Zn+2]': 123,\n",
       " '[Nd+3]': 124,\n",
       " '[Co+3]': 125,\n",
       " '[Ca]': 126,\n",
       " '[15n]': 127,\n",
       " '[CH2]': 128,\n",
       " '[C]': 129,\n",
       " '[CH]': 130,\n",
       " '[O]': 131,\n",
       " '%12': 132,\n",
       " '[OH+]': 133,\n",
       " '[OH2+]': 134,\n",
       " '[Cl]': 135,\n",
       " '[H+]': 136,\n",
       " '[NH]': 137,\n",
       " '[Cd]': 138,\n",
       " '[Cl+]': 139,\n",
       " '[Br+2]': 140,\n",
       " '[S+2]': 141,\n",
       " '[Cu+2]': 142,\n",
       " '[Mn+2]': 143,\n",
       " '[H-]': 144,\n",
       " '[SbH]': 145}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([156, 157, 158, 159, 160, 161, 162,   1,  13,   8,  12,  19,  20,\n",
       "        18,  13,  13,   8,  12,  13,  18,  19,  20,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " bert_tensor_small_1 (BERT_t  (None, 200, 256)         4519232   \n",
      " ensor_small)                                                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,519,232\n",
      "Trainable params: 4,519,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160, 161, 162, 163, 164, 165, 166,   1,  11,  10,  11,  12,  11,\n",
       "        11,  11,  12,  29,  18,  11,  10,  18,   8,  12,   8,   8,  13,\n",
       "        12,   8,  18,   8,  18,  11,  10,  11,  11,  11,  11,   9,  10,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 11:29:38.791655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-01-03 11:29:39.795269: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5630f753fc90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-03 11:29:39.795305: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-01-03 11:29:39.800980: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-03 11:29:39.944837: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f77fc701cf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f77fc701cf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Steps : 11969 Task : 0, Loss: 0.4417, acc : 0.7955, auc : 0.8660     Task : 1, Loss: 0.1973, acc : 0.9305, auc : 0.7510     Task : 2, Loss: 0.4085, acc : 0.8176, auc : 0.8451     Task : 3, Loss: 0.4782, acc : 0.7884, auc : 0.7505     Task : 4, Loss: 0.4839, acc : 0.7622, auc : 0.8411     Task : 5, Loss: 0.5215, acc : 0.7497, auc : 0.6939     Task : 6, Loss: 0.4428, acc : 0.7856, auc : 0.6214\n",
      "Task is AMES\n",
      "Test accuracy: 0.8255494236946106\n",
      "Test AUC: 0.89365553855896\n",
      "Test loss: 0.4091382324695587\n",
      "\n",
      "\n",
      "Task is ClinTox\n",
      "Test accuracy: 0.9324324131011963\n",
      "Test AUC: 0.8278035521507263\n",
      "Test loss: 0.2092052698135376\n",
      "\n",
      "\n",
      "Task is hERG\n",
      "Test accuracy: 0.8320610523223877\n",
      "Test AUC: 0.867479681968689\n",
      "Test loss: 0.4131025969982147\n",
      "\n",
      "\n",
      "Task is DILI\n",
      "Test accuracy: 0.75789475440979\n",
      "Test AUC: 0.8145518898963928\n",
      "Test loss: 0.5331652760505676\n",
      "\n",
      "\n",
      "Task is hERG_Karim\n",
      "Test accuracy: 0.7954630255699158\n",
      "Test AUC: 0.8796931505203247\n",
      "Test loss: 0.43764448165893555\n",
      "\n",
      "\n",
      "Task is Skin Reaction\n",
      "Test accuracy: 0.6913580298423767\n",
      "Test AUC: 0.727450966835022\n",
      "Test loss: 0.5942420959472656\n",
      "\n",
      "\n",
      "Task is Carcinogens_Lagunin\n",
      "Test accuracy: 0.9107142686843872\n",
      "Test AUC: 0.9170731902122498\n",
      "Test loss: 0.2834030091762543\n",
      "\n",
      "Epoch 2/25\n",
      "Steps : 6225 Task : 0, Loss: 0.3285, acc : 0.8613, auc : 0.9317     Task : 1, Loss: 0.1103, acc : 0.9581, auc : 0.8719     Task : 2, Loss: 0.3072, acc : 0.8798, auc : 0.9073     Task : 3, Loss: 0.3270, acc : 0.8860, auc : 0.8406     Task : 4, Loss: 0.3712, acc : 0.8342, auc : 0.9133     Task : 5, Loss: 0.4233, acc : 0.8114, auc : 0.7993     Task : 6, Loss: 0.2741, acc : 0.8995, auc : 0.7735"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn_reg = tf.keras.losses.MeanSquaredError()\n",
    "acc = np.array([0])\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    loss_list = [[] for i in range(len(Tox_names))]\n",
    "    acc_list = [[] for i in range(len(Tox_names))]\n",
    "    auc_list = [[] for i in range(len(Tox_names))]\n",
    "    for i in range(0, len(temp_zip)):\n",
    "        batch_images = x_trains[i]\n",
    "        batch_images = tf.reshape(batch_images,[-1,200])\n",
    "        batch_labels = y_trains[i]\n",
    "        batch_labels = tf.reshape(batch_labels,[-1])\n",
    "        task = task_labels[i]\n",
    "\n",
    "        if task != -1:\n",
    "            with tf.GradientTape() as tape:\n",
    "                model = globals()[f'Task{task}_model']\n",
    "                logits = model(batch_images)\n",
    "                logits = tf.reshape(logits,[-1])\n",
    "                loss_value = loss_fn(batch_labels, logits)\n",
    "                acc = tf.keras.metrics.Accuracy()(np.round(logits),batch_labels)\n",
    "                auc = tf.keras.metrics.AUC()(batch_labels,logits)\n",
    "                auc = auc.numpy()\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "                loss_list[task].append(loss_value)\n",
    "                acc_list[task].append(acc)\n",
    "                auc_list[task].append(auc)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        else:\n",
    "            with tf.GradientTape() as tape:\n",
    "                model = globals()[f'Task{task}_model']\n",
    "                logits = model(batch_images)\n",
    "                logits = tf.reshape(logits,[-1])\n",
    "                loss_value = loss_fn_reg(logits,batch_labels)\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "                loss_list[task].append(loss_value)\n",
    "                acc = tf.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None)(logits,batch_labels)\n",
    "                acc_list[task].append(acc)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        for l in range(len(Tox_names)):\n",
    "            \n",
    "            temp_loss = np.average(loss_list[l][-10000:])\n",
    "            temp_acc = np.average(acc_list[l][-10000:])\n",
    "            temp_auc = np.average(auc_list[l][-10000:])\n",
    "            if l == 0:\n",
    "                text = \"\\rSteps : {} Task : {}, Loss: {:.4f}, acc : {:.4f}, auc : {:.4f}\".format(i,l,temp_loss,temp_acc,temp_auc)\n",
    "            else:\n",
    "                text += \"     Task : {}, Loss: {:.4f}, acc : {:.4f}, auc : {:.4f}\".format(l,temp_loss,temp_acc,temp_auc)\n",
    "            sys.stdout.write(text)\n",
    "            sys.stdout.flush()\n",
    "    # 각 에포크 종료 후 평가\n",
    "    for temp_index,j in enumerate(val_task):\n",
    "        j = j\n",
    "        val_res = predict(globals()[f'Task{j}_model'],x_vals[temp_index],len_20s[temp_index])\n",
    "        val_res = val_res.reshape(-1)\n",
    "        if j != -1:\n",
    "            acc = tf.keras.metrics.Accuracy()(y_vals[temp_index],np.round(val_res))\n",
    "            auc_res = (AUC()(y_vals[temp_index],val_res)).numpy()\n",
    "            loss = loss_fn(y_vals[temp_index],val_res)\n",
    "            \n",
    "            print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "            print(f\"Test accuracy: {acc}\")\n",
    "            print(f\"Test AUC: {auc_res}\")\n",
    "            print(f\"Test loss: {loss}\\n\")\n",
    "        else:\n",
    "            loss = loss_fn_reg(y_vals[temp_index],val_res)\n",
    "            print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "            print(f\"\\nTest MSE:{loss}\")\n",
    "    for i in range(7):\n",
    "        globals()[f'Task{i}_model'].save_weights(f'./BERT/SMILE_MTL/BERT_model_task{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "    temp = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.46617275>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_res = predict(globals()[f'Task{2}_model'],x_vals[2],len_20s[2])\n",
    "val_res = val_res.reshape(-1)\n",
    "loss = loss_fn(y_vals[2],val_res)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.83206105>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.metrics.Accuracy()(y_vals[2],np.round(val_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vals[temp_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: '<unknown1>',\n",
       " 4: '<unknown2>',\n",
       " 5: '<unknown3>',\n",
       " 6: '<unknow4>',\n",
       " 7: '<unknown5>',\n",
       " 8: 'C',\n",
       " 9: 'n',\n",
       " 10: '1',\n",
       " 11: 'c',\n",
       " 12: '(',\n",
       " 13: 'N',\n",
       " 14: '2',\n",
       " 15: '[C@H]',\n",
       " 16: '[nH]',\n",
       " 17: 'S',\n",
       " 18: ')',\n",
       " 19: '=',\n",
       " 20: 'O',\n",
       " 21: '[C@@H]',\n",
       " 22: 'F',\n",
       " 23: '[O-]',\n",
       " 24: '[N@@H+]',\n",
       " 25: '3',\n",
       " 26: '[N@H+]',\n",
       " 27: 'o',\n",
       " 28: '4',\n",
       " 29: 'Br',\n",
       " 30: '[nH+]',\n",
       " 31: '[NH+]',\n",
       " 32: '5',\n",
       " 33: '[C@]',\n",
       " 34: '[C@@]',\n",
       " 35: 's',\n",
       " 36: '-',\n",
       " 37: '[NH3+]',\n",
       " 38: '/',\n",
       " 39: '\\\\',\n",
       " 40: 'Cl',\n",
       " 41: '[N+]',\n",
       " 42: '#',\n",
       " 43: '[N-]',\n",
       " 44: '[NH2+]',\n",
       " 45: '[P@]',\n",
       " 46: '[P@@]',\n",
       " 47: '[n-]',\n",
       " 48: '[n+]',\n",
       " 49: '[N@+]',\n",
       " 50: '[N@@+]',\n",
       " 51: 'I',\n",
       " 52: 'P',\n",
       " 53: '[PH+]',\n",
       " 54: '6',\n",
       " 55: '[S@@]',\n",
       " 56: '[Si]',\n",
       " 57: '[S@]',\n",
       " 58: '[o+]',\n",
       " 59: '[S-]',\n",
       " 60: '[O+]',\n",
       " 61: '7',\n",
       " 62: '[S@@+]',\n",
       " 63: '[S@+]',\n",
       " 64: '[S+]',\n",
       " 65: '[P@H]',\n",
       " 66: '[P@@H]',\n",
       " 67: '[SH]',\n",
       " 68: '[s+]',\n",
       " 69: '[P+]',\n",
       " 70: '[PH]',\n",
       " 71: '8',\n",
       " 72: '[Hg]',\n",
       " 73: '.',\n",
       " 74: '[Na]',\n",
       " 75: '[Cl+3]',\n",
       " 76: '[cH-]',\n",
       " 77: '[CH-]',\n",
       " 78: '[CH2-]',\n",
       " 79: '[C-]',\n",
       " 80: '[Cl+2]',\n",
       " 81: '[Cl-]',\n",
       " 82: '[Na+]',\n",
       " 83: '[NH4+]',\n",
       " 84: '[Br-]',\n",
       " 85: '[Li+]',\n",
       " 86: '[Gd+3]',\n",
       " 87: 'B',\n",
       " 88: '[Pt+2]',\n",
       " 89: '[Fe+4]',\n",
       " 90: '*',\n",
       " 91: '[H]',\n",
       " 92: '[N]',\n",
       " 93: '[99Tc]',\n",
       " 94: '[Se]',\n",
       " 95: '[Fe-2]',\n",
       " 96: '[Al]',\n",
       " 97: '[Ca+2]',\n",
       " 98: '[131I]',\n",
       " 99: '[Pt]',\n",
       " 100: '[Bi]',\n",
       " 101: '[123I]',\n",
       " 102: '[Au]',\n",
       " 103: '[201Tl]',\n",
       " 104: '[Cr]',\n",
       " 105: '[Cu]',\n",
       " 106: '[Mn]',\n",
       " 107: '[Zn]',\n",
       " 108: '[As]',\n",
       " 109: '[32P]',\n",
       " 110: '[Ti]',\n",
       " 111: '[I-]',\n",
       " 112: '9',\n",
       " 113: '%10',\n",
       " 114: '%11',\n",
       " 115: '[NH-]',\n",
       " 116: '[K+]',\n",
       " 117: '[B-]',\n",
       " 118: '[Mg+2]',\n",
       " 119: '[se]',\n",
       " 120: '[2H]',\n",
       " 121: '[Ba+2]',\n",
       " 122: '[OH-]',\n",
       " 123: '[Zn+2]',\n",
       " 124: '[Nd+3]',\n",
       " 125: '[Co+3]',\n",
       " 126: '[Ca]',\n",
       " 127: '[15n]',\n",
       " 128: '[CH2]',\n",
       " 129: '[C]',\n",
       " 130: '[CH]',\n",
       " 131: '[O]',\n",
       " 132: '%12',\n",
       " 133: '[OH+]',\n",
       " 134: '[OH2+]',\n",
       " 135: '[Cl]',\n",
       " 136: '[H+]',\n",
       " 137: '[NH]',\n",
       " 138: '[Cd]',\n",
       " 139: '[Cl+]',\n",
       " 140: '[Br+2]',\n",
       " 141: '[S+2]',\n",
       " 142: '[Cu+2]',\n",
       " 143: '[Mn+2]',\n",
       " 144: '[H-]',\n",
       " 145: '[SbH]'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {}\n",
    "\n",
    "\n",
    "for i in word2idx.keys():\n",
    "    idx2word[word2idx[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>[C@H]1([C@H](CC[C@H]2CN3[C@@H](C[C@H]12)c1c(c2c(cccc2)[nH]1)CC3)O)C(OC)=O\n",
      "<start>C1[C@H]2[C@H](C[C@H]3c4c(c5c(cccc5)[nH]4)CCN13)[C@H](C(OC)=O)[C@@H](O)CC2\n",
      "<start>c1cccc2c3CCN4C[C@H]5[C@H](C[C@H]4c3[nH]c12)[C@H](C(OC)=O)[C@H](CC5)O\n",
      "<start>c12[nH]c3cc(OC)ccc3c2CCN2[C@@H]1C[C@@H]1[C@H](C(O)=O)[C@@H](OC)[C@H](O)C[C@@H]1C2\n",
      "<start>[C@H]12C3CCC([C@H]1NCCC2)[C@H]1N3CCc2c3ccccc3[nH]c21\n",
      "<start>c1ccc(c2ccccc21)NCCN\n",
      "<start>c1cc(N2N=C(C(C2=O)/N=N/c2ccc(cc2)S(=O)(O)=O)C(O)=O)ccc1S(=O)(=O)O\n",
      "<start>N(\\c1ccc(cc1)S(=O)(=O)O)=N/C1C(=NN(C1=O)c1ccc(cc1)S(=O)(=O)O)C(=O)O\n"
     ]
    }
   ],
   "source": [
    "for index,i in enumerate(val_res):\n",
    "    if i>0.4 and i < 0.6:\n",
    "        temp = ''\n",
    "        for j in x_val[index]:\n",
    "            try:\n",
    "                temp+=idx2word[j]\n",
    "            except:\n",
    "                pass\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>C(\\C=O)=C\\O\n",
      "<start>O/C=C\\C=O\n",
      "<start>C(=O)/C=C\\O\n",
      "<start>[C@@H]1(CC[C@H]2CN3CCc4c5c([nH]c4[C@@H]3C[C@@H]2[C@@H]1C(OC)=O)cccc5)O\n",
      "<start>C1CN2C[C@H]3[C@@H]([C@@H]([C@H](CC3)O)C(=O)OC)C[C@H]2c2[nH]c3ccccc3c12\n",
      "<start>[C@H]12[C@@H](CC[C@@H]([C@H]1C(OC)=O)O)CN1[C@@H](C2)c2c(c3c(cccc3)[nH]2)CC1\n",
      "<start>C([C@H]1[C@H]2C[C@@H]3N(CCc4c5c([nH]c43)cccc5)C[C@@H]2CC[C@@H]1O)(=O)OC\n",
      "<start>c1c2c3c([C@H]4N(CC3)C[C@@H]3CC[C@H](O)[C@H]([C@H]3C4)C(OC)=O)[nH]c2ccc1\n",
      "<start>COC([C@@H]1[C@H](CC[C@@H]2[C@@H]1C[C@@H]1N(CCc3c1[nH]c1c3cccc1)C2)O)=O\n",
      "<start>c12c3c([C@@H]4C[C@@H]5[C@@H]([C@@H](O)CC[C@H]5CN4CC3)C(OC)=O)[nH]c2cccc1\n",
      "<start>[C@H]1(O)CC[C@H]2CN3[C@@H](C[C@@H]2[C@@H]1C(OC)=O)c1[nH]c2c(cccc2)c1CC3\n",
      "<start>O[C@H]1CC[C@@H]2[C@@H]([C@@H]1C(OC)=O)C[C@H]1c3[nH]c4ccccc4c3CCN1C2\n",
      "<start>[C@@H]1(O)[C@@H](C(OC)=O)[C@@H]2[C@H](CN3[C@@H](C2)c2[nH]c4ccccc4c2CC3)CC1\n",
      "<start>O=C(OC)[C@@H]1[C@@H](O)CC[C@@H]2[C@@H]1C[C@@H]1N(C2)CCc2c3ccccc3[nH]c12\n",
      "<start>C1[C@@H]2c3[nH]c4cc(OC)ccc4c3CCN2C[C@@H]2[C@H]1[C@H](C(=O)O)[C@H]([C@H](O)C2)OC\n",
      "<start>c1(ccc2c3CCN4C[C@@H]5[C@@H]([C@@H]([C@@H](OC)[C@H](O)C5)C(=O)O)C[C@@H]4c3[nH]c2c1)OC\n",
      "<start>c1cc(cc2c1c1CCN3[C@H](C[C@@H]4[C@@H]([C@@H](OC)[C@@H](C[C@@H]4C3)O)C(=O)O)c1[nH]2)OC\n",
      "<start>[C@H]12[C@H](C[C@H]3N(C1)CCc1c3[nH]c3cc(ccc31)OC)[C@@H]([C@H]([C@H](O)C2)OC)C(O)=O\n",
      "<start>O=C(O)[C@H]1[C@H]2C[C@@H]3c4c(CCN3C[C@H]2C[C@@H](O)[C@@H]1OC)c1ccc(OC)cc1[nH]4\n",
      "<start>C([C@@H]1[C@@H](OC)[C@H](O)C[C@H]2[C@@H]1C[C@@H]1c3c(CCN1C2)c1ccc(OC)cc1[nH]3)(=O)O\n",
      "<start>[C@H]1(C(O)=O)[C@@H]2[C@@H](CN3CCc4c5ccc(OC)cc5[nH]c4[C@H]3C2)C[C@@H](O)[C@@H]1OC\n",
      "<start>c12c3CCN4[C@@H](c3[nH]c1cccc2)C1CCC4[C@@H]2CCCN[C@@H]21\n",
      "<start>C1C2[C@H]3NCCC[C@H]3C(N3CCc4c5ccccc5[nH]c4[C@H]32)C1\n",
      "<start>C12CCC(N3CCc4c5c(cccc5)[nH]c4[C@@H]23)[C@H]2[C@@H]1NCCC2\n",
      "<start>C1N[C@H]2[C@@H](CC1)C1CCC2[C@@H]2c3[nH]c4ccccc4c3CCN21\n",
      "<start>C1N[C@H]2[C@H](C3CCC2[C@@H]2c4c(c5ccccc5[nH]4)CCN23)CC1\n",
      "<start>C1Cc2c([nH]c3ccccc23)[C@@H]2N1C1CCC2[C@H]2NCCC[C@H]21\n",
      "<start>c1ccc2[nH]c3c(CCN4C5CCC([C@H]6NCCC[C@H]65)[C@H]34)c2c1\n",
      "<start>[C@H]12[C@@H](CCCN2)C2N3[C@H](C1CC2)c1[nH]c2c(c1CC3)cccc2\n",
      "<start>C1C[C@@H]2[C@@H](C3[C@H]4N(CCc5c6c(cccc6)[nH]c54)C2CC3)NC1\n",
      "<start>C12CCC([C@H]3NCCC[C@@H]13)[C@@H]1c3c(CCN21)c1c([nH]3)cccc1\n",
      "<start>[C@H]12NCCC[C@H]1C1N3CCc4c5c(cccc5)[nH]c4[C@H]3C2CC1\n",
      "<start>C1CC[C@@H]2[C@H](N1)C1CCC2N2CCc3c([C@@H]12)[nH]c1c3cccc1\n",
      "<start>c1(/C=C/C(O[C@H]2[C@@H]([C@H](O)C[C@](C2)(C(=O)O)O)O)=O)cc(c(O)cc1)O\n",
      "<start>C(O)(=O)[C@]1(O)C[C@@H](O)[C@@H](O)[C@@H](C1)OC(/C=C/c1ccc(c(O)c1)O)=O\n",
      "<start>c1(O)c(O)ccc(c1)/C=C/C(=O)O[C@@H]1C[C@@](O)(C[C@@H](O)[C@H]1O)C(O)=O\n",
      "<start>O[C@@H]1C[C@](C[C@@H](OC(=O)/C=C/c2cc(O)c(O)cc2)[C@@H]1O)(C(=O)O)O\n",
      "<start>O=C(O)[C@]1(C[C@H]([C@@H]([C@@H](C1)O)O)OC(/C=C/c1cc(O)c(O)cc1)=O)O\n",
      "<start>[C@@H]1(O)[C@@H](C[C@@](C[C@H]1OC(/C=C/c1cc(O)c(cc1)O)=O)(O)C(=O)O)O\n",
      "<start>c1(O)c(cc(cc1)/C=C/C(O[C@H]1[C@@H]([C@@H](C[C@](C(O)=O)(O)C1)O)O)=O)O\n",
      "<start>C1[C@](O)(C(O)=O)C[C@H]([C@@H](O)[C@@H]1OC(/C=C/c1ccc(c(O)c1)O)=O)O\n",
      "<start>c1c(/C=C/C(O[C@H]2[C@@H]([C@H](O)C[C@](C2)(C(=O)O)O)O)=O)ccc(O)c1O\n",
      "<start>[C@H]1(C[C@](C(=O)O)(O)C[C@@H](OC(/C=C/c2cc(O)c(O)cc2)=O)[C@@H]1O)O\n",
      "<start>C1[C@H]([C@@H]([C@@H](C[C@@]1(O)C(=O)O)O)O)OC(/C=C/c1ccc(c(c1)O)O)=O\n",
      "<start>C(O[C@@H]1C[C@](C(=O)O)(C[C@@H](O)[C@H]1O)O)(=O)/C=C/c1ccc(O)c(c1)O\n",
      "<start>c1ccc2c(c1NCCN)cccc2\n",
      "<start>c1cc2cccc(NCCN)c2cc1\n",
      "<start>c1c(c2c(cc1)cccc2)NCCN\n",
      "<start>c1cccc2c1cccc2NCCN\n",
      "<start>c12c(cccc2)cccc1NCCN\n",
      "<start>C(CNc1c2ccccc2ccc1)N\n",
      "<start>c12ccccc1cccc2NCCN\n",
      "<start>c1c2ccccc2c(NCCN)cc1\n",
      "<start>c12cccc(NCCN)c2cccc1\n",
      "<start>N(CCN)c1c2ccccc2ccc1\n",
      "<start>c1cc(c2ccccc2c1)NCCN\n",
      "<start>C1(/N=N/c2ccc(cc2)S(=O)(=O)O)C(C(O)=O)=NN(c2ccc(cc2)S(=O)(=O)O)C1=O\n",
      "<start>C1(/N=N/c2ccc(S(O)(=O)=O)cc2)C(N(N=C1C(=O)O)c1ccc(S(=O)(O)=O)cc1)=O\n",
      "<start>N(\\c1ccc(cc1)S(O)(=O)=O)=N/C1C(=O)N(c2ccc(cc2)S(=O)(O)=O)N=C1C(O)=O\n",
      "<start>N1=C(C(O)=O)C(/N=N/c2ccc(cc2)S(=O)(=O)O)C(N1c1ccc(cc1)S(O)(=O)=O)=O\n",
      "<start>O=C(C1C(C(N(N=1)c1ccc(cc1)S(O)(=O)=O)=O)/N=N/c1ccc(S(=O)(O)=O)cc1)O\n",
      "<start>c1c(ccc(c1)S(=O)(=O)O)/N=N/C1C(C(=O)O)=NN(c2ccc(cc2)S(=O)(O)=O)C1=O\n",
      "<start>c1cc(ccc1N1N=C(C(O)=O)C(/N=N/c2ccc(cc2)S(O)(=O)=O)C1=O)S(O)(=O)=O\n",
      "<start>N1(C(C(C(=N1)C(O)=O)/N=N/c1ccc(S(=O)(=O)O)cc1)=O)c1ccc(cc1)S(O)(=O)=O\n",
      "<start>C(=O)(C1C(/N=N/c2ccc(cc2)S(O)(=O)=O)C(N(c2ccc(cc2)S(O)(=O)=O)N=1)=O)O\n",
      "<start>O=S(O)(c1ccc(cc1)N1N=C(C(=O)O)C(/N=N/c2ccc(S(O)(=O)=O)cc2)C1=O)=O\n",
      "<start>OS(=O)(c1ccc(N2C(=O)C(C(=N2)C(=O)O)/N=N/c2ccc(cc2)S(=O)(=O)O)cc1)=O\n",
      "<start>S(O)(c1ccc(/N=N/C2C(N(N=C2C(O)=O)c2ccc(cc2)S(=O)(=O)O)=O)cc1)(=O)=O\n",
      "<start>c1cc(ccc1/N=N/C1C(N(N=C1C(=O)O)c1ccc(cc1)S(O)(=O)=O)=O)S(O)(=O)=O\n",
      "<start>c12c3C[C@H]4N(C)C[C@H](C(N[C@@H](CC)CO)=O)C=C4c2cccc1[nH]c3\n",
      "<start>c1cc2c3c(C[C@@H]4C2=C[C@H](CN4C)C(=O)N[C@@H](CC)CO)c[nH]c3c1\n",
      "<start>c1cc2C3=C[C@@H](C(=O)N[C@@H](CC)CO)CN([C@@H]3Cc3c2c([nH]c3)c1)C\n"
     ]
    }
   ],
   "source": [
    "for index,i in enumerate(val_res):\n",
    "    if i > 0.9:\n",
    "        temp = ''\n",
    "        for j in x_val[index]:\n",
    "            try:\n",
    "                temp+=idx2word[j]\n",
    "            except:\n",
    "                pass\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task is AMES\n",
      "Test accuracy: 0.7767857313156128\n",
      "Test AUC: 0.8451955318450928\n",
      "Test loss: 0.48885905742645264\n",
      "\n",
      "\n",
      "Task is ClinTox\n",
      "Test accuracy: 0.9256756901741028\n",
      "Test AUC: 0.7087756991386414\n",
      "Test loss: 0.25217902660369873\n",
      "\n",
      "\n",
      "Task is hERG\n",
      "Test accuracy: 0.7938931584358215\n",
      "Test AUC: 0.85514897108078\n",
      "Test loss: 0.48679855465888977\n",
      "\n",
      "\n",
      "Task is DILI\n",
      "Test accuracy: 0.7157894968986511\n",
      "Test AUC: 0.8307453393936157\n",
      "Test loss: 0.5598265528678894\n",
      "\n",
      "\n",
      "Task is hERG_Karim\n",
      "Test accuracy: 0.7188546061515808\n",
      "Test AUC: 0.7918149828910828\n",
      "Test loss: 0.5562483668327332\n",
      "\n",
      "\n",
      "Task is Skin Reaction\n",
      "Test accuracy: 0.6419752836227417\n",
      "Test AUC: 0.6545751690864563\n",
      "Test loss: 0.616245687007904\n",
      "\n",
      "\n",
      "Task is Carcinogens_Lagunin\n",
      "Test accuracy: 0.8928571343421936\n",
      "Test AUC: 0.9235773086547852\n",
      "Test loss: 0.3122290074825287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j,j in enumerate(val_task):\n",
    "    val_res = predict(globals()[f'Task{j}_model'],x_vals[j],len_20s[j])\n",
    "    if j != -1:\n",
    "        acc = tf.keras.metrics.Accuracy()(y_vals[j],np.round(val_res))\n",
    "        auc_res = (AUC()(y_vals[j],val_res)).numpy()\n",
    "        loss = tf.keras.metrics.BinaryCrossentropy()(y_vals[j],val_res)\n",
    "        \n",
    "        print(f'\\nTask is {Tox_names[j]}')\n",
    "        print(f\"Test accuracy: {acc}\")\n",
    "        print(f\"Test AUC: {auc_res}\")\n",
    "        print(f\"Test loss: {loss}\\n\")\n",
    "    else:\n",
    "        loss = tf.keras.metrics.MeanSquaredError()(y_vals[j],val_res)\n",
    "        print(f'\\nTask is {Tox_names[j]}')\n",
    "        print(f\"\\nTest MSE:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
