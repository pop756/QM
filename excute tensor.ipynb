{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import execute_tensor as execute\n",
    "from execute_tensor import execute as exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected :  [Cu+2]\n",
      "Unexpected :  [Mn+2]\n",
      "Unexpected :  [H-]\n",
      "Unexpected :  [SbH]\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tox_name \u001b[38;5;129;01min\u001b[39;00m tox_names:\n\u001b[1;32m      9\u001b[0m     tox_execute \u001b[38;5;241m=\u001b[39m exe(tox_name,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m552\u001b[39m,epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,tokens\u001b[38;5;241m=\u001b[39mtokens)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtox_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtox_execute_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtox_name] \u001b[38;5;241m=\u001b[39m tox_execute\n\u001b[1;32m     13\u001b[0m     models \u001b[38;5;241m=\u001b[39m {tox_execute\u001b[38;5;241m.\u001b[39mBit:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBit_Classifier\u001b[39m\u001b[38;5;124m'\u001b[39m,tox_execute\u001b[38;5;241m.\u001b[39mBERTs[\u001b[38;5;241m0\u001b[39m]:BERT_names[\u001b[38;5;241m0\u001b[39m],tox_execute\u001b[38;5;241m.\u001b[39mBERTs[\u001b[38;5;241m1\u001b[39m]:BERT_names[\u001b[38;5;241m1\u001b[39m]}\n",
      "File \u001b[0;32m~/Downloads/Qunova-machine/execute_tensor.py:180\u001b[0m, in \u001b[0;36mexecute.forward\u001b[0;34m(self, set_weights)\u001b[0m\n\u001b[1;32m    176\u001b[0m val_call \u001b[38;5;241m=\u001b[39m CustomCallback(x_val,y_val,len_20)\n\u001b[1;32m    178\u001b[0m temp_BERT\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m),loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m,AUC(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m--> 180\u001b[0m hist1 \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_BERT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mval_call\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m temp_BERT\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_call\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    184\u001b[0m temp_BERT\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_call\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:945\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    948\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    949\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    950\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tox_names = ['hERG','AMES',\"Carcinogens_Lagunin\",'ClinTox','hERG_Karim']\n",
    "tokens = ['SMILE','AIS']\n",
    "BERT_names = []\n",
    "\n",
    "for token in tokens:\n",
    "    BERT_names.append(token+\"_BERT_Classifier\")\n",
    "\n",
    "for tox_name in tox_names:\n",
    "    tox_execute = exe(tox_name,0.2,552,epoch=20,batch=32,tokens=tokens)\n",
    "    tox_execute.forward()\n",
    "    globals()['tox_execute_'+tox_name] = tox_execute\n",
    "    \n",
    "    models = {tox_execute.Bit:'Bit_Classifier',tox_execute.BERTs[0]:BERT_names[0],tox_execute.BERTs[1]:BERT_names[1]}\n",
    "    execute.plot_history(models,tox_name,['SMILE_AIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 93/328 [=======>......................] - ETA: 44s - loss: 0.5295 - acc: 0.7476 - auc: 0.7370"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tox_name \u001b[38;5;129;01min\u001b[39;00m tox_names:\n\u001b[1;32m      9\u001b[0m     tox_execute \u001b[38;5;241m=\u001b[39m exe(tox_name,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m552\u001b[39m,epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,tokens\u001b[38;5;241m=\u001b[39mtokens)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtox_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtox_execute_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtox_name] \u001b[38;5;241m=\u001b[39m tox_execute\n\u001b[1;32m     13\u001b[0m     models \u001b[38;5;241m=\u001b[39m {tox_execute\u001b[38;5;241m.\u001b[39mBit:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBit_Classifier\u001b[39m\u001b[38;5;124m'\u001b[39m,tox_execute\u001b[38;5;241m.\u001b[39mBERTs[\u001b[38;5;241m0\u001b[39m]:BERT_names[\u001b[38;5;241m0\u001b[39m],tox_execute\u001b[38;5;241m.\u001b[39mBERTs[\u001b[38;5;241m1\u001b[39m]:BERT_names[\u001b[38;5;241m1\u001b[39m]}\n",
      "File \u001b[0;32m~/Downloads/Qunova-machine/execute_tensor.py:180\u001b[0m, in \u001b[0;36mexecute.forward\u001b[0;34m(self, set_weights)\u001b[0m\n\u001b[1;32m    176\u001b[0m val_call \u001b[38;5;241m=\u001b[39m CustomCallback(x_val,y_val,len_20)\n\u001b[1;32m    178\u001b[0m temp_BERT\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m),loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m,AUC(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m--> 180\u001b[0m hist1 \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_BERT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mval_call\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m temp_BERT\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_call\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    184\u001b[0m temp_BERT\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_call\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tox_names = ['hERG','AMES',\"Carcinogens_Lagunin\",'ClinTox','hERG_Karim']\n",
    "BERT_names = []\n",
    "\n",
    "for token in tokens:\n",
    "    BERT_names.append(token+\"_BERT_Classifier\")\n",
    "\n",
    "for tox_name in tox_names:\n",
    "    tox_execute = exe(tox_name,0.2,552,epoch=20,batch=32,tokens=tokens)\n",
    "    tox_execute.forward()\n",
    "    globals()['tox_execute_'+tox_name] = tox_execute\n",
    "    \n",
    "    models = {tox_execute.Bit:'Bit_Classifier',tox_execute.BERTs[0]:BERT_names[0],tox_execute.BERTs[1]:BERT_names[1]}\n",
    "    execute.plot_history(models,tox_name,['SMILE_AIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module import custom_layers\n",
    "import tensorflow as tf\n",
    "\n",
    "BERT_tensor = custom_layers.BERT_tensor\n",
    "\n",
    "bert_layer = BERT_tensor(256,6,1024,strat_index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "inputs = tf.keras.layers.Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = bert_layer(inputs,None)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./BERT/atomInSmile/small_tensor_Pre_BERT')\n",
    "paras = model.get_weights()\n",
    "bert_layer.set_weights(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Task_mask(num_task):\n",
    "    result = np.zeros([200,200])\n",
    "    for i in range(num_task):\n",
    "        for j in range(200):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                result[j][i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Task_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.metrics import AUC\n",
    "Models = []\n",
    "\n",
    "Input = tf.keras.layers.Input(200,)\n",
    "hidden = bert_layer(Input,att_mask = mask)\n",
    "output = hidden[:,0]\n",
    "output = tf.keras.layers.Dense(256,activation = 'gelu')(output)\n",
    "output = tf.keras.layers.Dense(1)(output)\n",
    "globals()[f'Task{0}_model'] = Model(inputs = [Input],outputs = [output])\n",
    "globals()[f'Task{0}_model'].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
    "Models.append(globals()[f'Task{0}_model'])\n",
    "\n",
    "for i in range(9):\n",
    "    i = i+1\n",
    "    Input = tf.keras.layers.Input(200,)\n",
    "    hidden = bert_layer(Input,att_mask = mask)\n",
    "    output = hidden[:,i]\n",
    "    output = tf.keras.layers.Dense(256,activation = 'gelu')(output)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(output)\n",
    "    globals()[f'Task{i}_model'] = Model(inputs = [Input],outputs = [output])\n",
    "    globals()[f'Task{i}_model'].compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "    Models.append(globals()[f'Task{i}_model'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tdc.single_pred import Tox\n",
    "from Module import RDK\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,mask_zero = True)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = np.array([0]+[0]+[i+2 for i in range(198)])\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions    \n",
    "    \n",
    "    \n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        l2_reg = tf.keras.regularizers.l2(0.01)\n",
    "        \n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim,kernel_regularizer=l2_reg)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs,attention_mask=tf.cast(np.array([[1]+[0]+[1]*198] + [[0]+[1]*199] + [[0]*2+[1]*198]*198),bool))\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([TransformerBlock(emb_dim,num_heads,ff_dim) for i in range(8)])\n",
    "        \n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = tf.keras.layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = tf.keras.layers.Dense(71,activation = 'softmax')\n",
    "    def call(self, inputs, mask_index=None,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "        \n",
    "        inputs = tf.reshape(inputs,[-1,200])\n",
    "        hidden = self.embedding(inputs)\n",
    "        \n",
    "        \n",
    "        hidden = self.encoder(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden\n",
    "\n",
    "\n",
    "def predict(model,results,len_list):\n",
    "    index = 0\n",
    "    res = model.predict(results,verbose=0)\n",
    "    x_val = []\n",
    "    for i in len_list:\n",
    "        temp = res[index:index+i]\n",
    "        x_val.append(np.average(temp,axis=0))\n",
    "        index = index+i\n",
    "    return np.array(x_val)\n",
    "\n",
    "def similar(a, b):    return SequenceMatcher(None, a, b).ratio()\n",
    "def most_similar(query,word2idx):\n",
    "\n",
    "    max = 0\n",
    "    tokken = ''\n",
    "    query = query.split(';')\n",
    "    for i in word2idx.keys():\n",
    "        key = i.split(';')\n",
    "        temp2 = 0\n",
    "        temp3 = 0\n",
    "        temp1 = similar(query[0],key[0])*10\n",
    "        try:\n",
    "            temp2 = similar(query[1],key[1])*2\n",
    "            temp3 = similar(query[2],key[2])*1\n",
    "        except:\n",
    "            pass\n",
    "        temp = temp1+temp2+temp3\n",
    "        if temp>max:\n",
    "            max = temp\n",
    "            tokken = i\n",
    "    return tokken\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_val,y_true,len_20):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        index = 0\n",
    "        res = []\n",
    "        for i in len_20:\n",
    "            res.append(np.average(y_true[index:index+i]))\n",
    "            index = index+i\n",
    "        self.len20 = len_20\n",
    "        self.counts = []\n",
    "        self.max = 0\n",
    "        self.y_true = np.array(res)\n",
    "        self.history = {}\n",
    "        self.epoch = 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 에포크가 끝날 때마다 validation 데이터로 모델 평가\n",
    "        result = predict(self.model,self.x_val,self.len20)\n",
    "        acc = Accuracy()(self.y_true,np.round(result))\n",
    "        auc_res = (AUC()(self.y_true,result)).numpy()\n",
    "        loss = tf.keras.metrics.BinaryCrossentropy()(self.y_true,result)\n",
    "        auc_res = auc_res\n",
    "        print(f\"     val_acc : {acc},    val_auc : {auc_res} val_loss : {loss}\")\n",
    "        if 'val_acc' not in self.history:\n",
    "            self.history['val_acc'] = [acc]\n",
    "        else:\n",
    "            self.history['val_acc'] += [acc]\n",
    "        if 'val_auc' not in self.history:\n",
    "            self.history['val_auc'] = [auc_res]\n",
    "        else:\n",
    "            self.history['val_auc'] += [auc_res]\n",
    "        \n",
    "        if 'val_loss' not in self.history:\n",
    "            self.history['val_loss'] = [loss]\n",
    "        else:\n",
    "            self.history['val_loss'] += [loss]\n",
    "        \n",
    "        self.max = np.max(self.history['val_auc'])\n",
    "            \n",
    "        if self.history['val_auc'][-1]<self.max:\n",
    "            self.counts.append(1)\n",
    "        else:\n",
    "            self.counts = []\n",
    "\n",
    "        self.epoch += 1\n",
    "        \"\"\"\n",
    "        if self.epoch>10 and len(self.counts)>2:\n",
    "            self.model.stop_training = True\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = BERT(256,6,1024)(inputs,None)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "#model.load_weights('./BERT/atomInSmile/F_Random_ZINC_L_model_weights.h5')\n",
    "\n",
    "\n",
    "#BERT_parameters = model.get_weights()[:130]\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def BERT_model():\n",
    "    inputs = Input(200,)\n",
    "    hidden = bert_layer(inputs)\n",
    "    hidden = hidden[:,0]\n",
    "    hidden = tf.keras.layers.Dense(256,activation = 'gelu')(hidden)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(hidden)\n",
    "    result = Model(inputs = [inputs],outputs = [output])\n",
    "    return result\n",
    "\n",
    "def Bit_model():\n",
    "    inputs = Input(2048,)\n",
    "    hidden = tf.keras.layers.Dense(250,activation = 'relu')(inputs)\n",
    "    hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    hidden = tf.keras.layers.Dense(40,activation = 'relu')(hidden)\n",
    "    hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    hidden = tf.keras.layers.Dense(10,activation = 'relu')(hidden)\n",
    "    hidden = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "    output = tf.keras.layers.Dense(1,activation = 'sigmoid')(hidden)\n",
    "    result = Model(inputs = [inputs],outputs = [output])\n",
    "    return result\n",
    "\n",
    "\n",
    "    \n",
    "class tox_process():\n",
    "    def __init__(self,tox,test_size=0.2,random_state = 1024):\n",
    "        self.tox_name = tox\n",
    "        self.size = test_size\n",
    "        self.seed = random_state\n",
    "        \n",
    "        \n",
    "    def AIS_process(self,plot=False,token = 'AIS',number_of_task = 2):\n",
    "        if token == 'AIS':\n",
    "            with open('./Tox_data/AIS_Tox_data/'+self.tox_name,'rb') as file:\n",
    "                train,label,len_20 = pickle.load(file)[0]\n",
    "            with open('./BERT/atomInSmile/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "                word2idx = pickle.load(file)\n",
    "        elif token == 'SMILE':\n",
    "            with open('./Tox_data/SMILE_Tox_data/'+self.tox_name,'rb') as file:\n",
    "                train,label,len_20 = pickle.load(file)[0]\n",
    "            with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "                word2idx = pickle.load(file)\n",
    "                \n",
    "            \"\"\"elif token == 'SmiletoPE':\n",
    "                with open('./Tox_data/SmiletoPE/'+self.tox_name,'rb') as file:\n",
    "                    train,label,len_20 = pickle.load(file)[0]\n",
    "                with open('./BERT/SmiletoPE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "                    word2idx = pickle.load(file)\"\"\"\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            temp_dict = {}\n",
    "\n",
    "            for i in train:\n",
    "                try:\n",
    "                    temp_dict[len(i)] = temp_dict[len(i)] + 1\n",
    "                except:\n",
    "                    temp_dict[len(i)] = 1\n",
    "                    \n",
    "            plt.bar(temp_dict.keys(),temp_dict.values())\n",
    "        \n",
    "        except_dict = {}\n",
    "\n",
    "        for i in train:\n",
    "            for j in i:\n",
    "                try:\n",
    "                    word2idx[j]\n",
    "                except:\n",
    "                    try:\n",
    "                        except_dict[j]\n",
    "                    except:\n",
    "                        except_dict[j] = len(except_dict) + 1\n",
    "        \n",
    "        similar_dict = {}\n",
    "\n",
    "        for i in except_dict.keys():\n",
    "            similar_dict[i] = most_similar(i,word2idx)\n",
    "            \n",
    "            \n",
    "        AIS_train = []\n",
    "        for index,i in enumerate(train):\n",
    "            \n",
    "            temp = []\n",
    "            if number_of_task == 2:\n",
    "                temp.append(3)\n",
    "                temp.append(4)\n",
    "            else:\n",
    "                for k in range(number_of_task):\n",
    "                    temp.append(len(word2idx)+1+k)\n",
    "            temp.append(1)\n",
    "            for j in i:\n",
    "                try:\n",
    "                    temp.append(word2idx[j])\n",
    "                except:\n",
    "                    print('Unexpected : ',j)\n",
    "                    word2idx[j] = len(word2idx)+1\n",
    "                    temp.append(word2idx[j])\n",
    "                \"\"\"    \n",
    "                except:\n",
    "                    if j != '/[H]':\n",
    "                        print(j,i)\n",
    "                        word_sim = similar_dict[j]\n",
    "                        if word_sim != '':\n",
    "                            temp.append(word2idx[word_sim])\n",
    "                    else:\n",
    "                        pass\"\"\"\n",
    "            if len(temp)>1:\n",
    "                AIS_train.append(temp)\n",
    "\n",
    "        AIS_train = tf.keras.preprocessing.sequence.pad_sequences(AIS_train, padding='post', maxlen=200)\n",
    "        temp_x = []\n",
    "        temp_y = []\n",
    "        index = 0\n",
    "        for i in len_20:\n",
    "            temp_x.append(AIS_train[index:index+i])\n",
    "            temp_y.append(label[index:index+i])\n",
    "            index = index+i\n",
    "\n",
    "\n",
    "\n",
    "        x_train, x_val, y_train, y_val,_,len_20 = train_test_split(temp_x,temp_y,len_20, test_size=self.size,random_state=self.seed)\n",
    "        \n",
    "        def flatten(data):\n",
    "            temp = []\n",
    "            for i in data:\n",
    "                temp+=list(i)\n",
    "            data = np.array(temp)\n",
    "            return data\n",
    "        x_train = flatten(x_train)\n",
    "        x_val = flatten(x_val)\n",
    "        y_val = flatten(y_val)\n",
    "        y_train = flatten(y_train)\n",
    "        return x_train, x_val, y_train, y_val,len_20\n",
    "    \n",
    "    def bit_precess(self):\n",
    "        train,tox_info = Tox(name=self.tox_name).get_data(format='DeepPurpose')\n",
    "        bit_string = RDK.smile_to_RDkit(train,2048)\n",
    "        x_train_NN,x_val_NN,y_train_NN,y_val_NN = train_test_split(np.array(bit_string)/1.,np.array(tox_info)/1.,test_size=self.size,random_state=self.seed)\n",
    "        return x_train_NN,x_val_NN,y_train_NN,y_val_NN\n",
    "    \n",
    "\n",
    "\n",
    "class execute():\n",
    "    def __init__(self,tox,test_size,split_seed,epoch = 20,batch=32*20,tokens = ['AIS']):\n",
    "        super().__init__()\n",
    "        self.tox = tox\n",
    "        self.size = test_size\n",
    "        self.seed = split_seed\n",
    "        self.epoch = epoch\n",
    "        self.BERTs = []\n",
    "        self.batch_size = batch\n",
    "        Bit_Classifier = Bit_model()\n",
    "        self.Bit = Bit_Classifier\n",
    "        #model.load_weights('./BERT/atomInSmile/F_Random_ZINC_L_model_weights.h5')\n",
    "        self.tokens = tokens\n",
    "        self.BERT_parameters = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token == 'AIS':\n",
    "                model.load_weights('./BERT/atomInSmile/Pre_BERT.h5')\n",
    "            elif token == 'SMILE':\n",
    "                model.load_weights('./BERT/SMILE/Pre_BERT.h5')\n",
    "            elif token == 'SmiletoPE':\n",
    "                model.load_weights('./BERT/SmiletoPE/F_Random_ZINC_L_model_weights.h5')\n",
    "            else:\n",
    "                raise\n",
    "            self.BERTs.append(BERT_model())\n",
    "            self.BERT_parameters.append(model.get_weights()[:130])\n",
    "    def forward(self,set_weights=True):\n",
    "        if set_weights:\n",
    "            for index,token in enumerate(self.tokens):\n",
    "                BERT_parameter = self.BERT_parameters[index]\n",
    "                self.BERTs[index].layers[1].set_weights(BERT_parameter)\n",
    "        \n",
    "\n",
    "        for index,token in enumerate(self.tokens): \n",
    "            process = tox_process(self.tox, self.size, self.seed)\n",
    "            x_train, x_val, y_train, y_val,len_20 = process.AIS_process(token = token)\n",
    "            \n",
    "            temp_BERT = self.BERTs[index]\n",
    "            \n",
    "            val_call = CustomCallback(x_val,y_val,len_20)\n",
    "            \n",
    "            temp_BERT.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "            \n",
    "            hist1 = temp_BERT.fit(x_train,y_train,batch_size=self.batch_size,epochs=self.epoch,callbacks=[val_call])\n",
    "            \n",
    "        \n",
    "            temp_BERT.history.history['val_loss'] = val_call.history['val_loss']\n",
    "            temp_BERT.history.history['val_acc'] = val_call.history['val_acc']\n",
    "            temp_BERT.history.history['val_auc'] = val_call.history['val_auc']\n",
    "            \n",
    "        x_train_NN,x_val_NN,y_train_NN,y_val_NN = process.bit_precess()\n",
    "        self.Bit.compile(optimizer = 'Adam',loss = 'binary_crossentropy',metrics=['acc',AUC(name='auc')])\n",
    "        hist2 = self.Bit.fit(x_train_NN,y_train_NN,batch_size=32,epochs=self.epoch,validation_data=(x_val_NN,y_val_NN))\n",
    "        \n",
    "        \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(models,tox_name,token=['AIS']):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['loss']))],model.history.history['loss'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_loss']))],val_call2.history.history['val_loss'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' train Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 2)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['acc']))],model.history.history['acc'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_acc']))],val_call2.history.history['val_acc'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' train ACC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 3)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['auc']))],model.history.history['auc'],label=models[model])\n",
    "    plt.title(tox_name + ' train AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['val_loss']))],model.history.history['val_loss'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_loss']))],val_call2.history.history['val_loss'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' val Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 5)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['val_acc']))],model.history.history['val_acc'],label=models[model])\n",
    "    #plt.plot([i for i in range(len(val_call2.history.history['val_acc']))],val_call2.history.history['val_acc'],label = 'BERT_Norm acc data')\n",
    "    plt.title(tox_name + ' val ACC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend('epoch')\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout() \n",
    "    plt.subplot(2, 3, 6)\n",
    "    for model in models.keys():\n",
    "        plt.plot([i for i in range(len(model.history.history['val_auc']))],model.history.history['val_auc'],label=models[model])\n",
    "    plt.title(tox_name + ' val AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('score')\n",
    "    \n",
    "    plt.savefig(f'./Results/{token[0]}_{token[1]}_Tox_result/'+tox_name+'.png')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "Tox_names = ['LD50_Zhu','AMES','ClinTox','hERG','DILI','Skin_Reaction','hERG_Karim']\n",
    "x_trains = [] \n",
    "y_trains = []\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "len_20s = []\n",
    "task_labels = []\n",
    "val_task = []\n",
    "for index1,i in enumerate(Tox_names):\n",
    "    try:\n",
    "        \n",
    "        A = tox_process(i)\n",
    "        x_train,x_val,y_train,y_val,len_20 = A.AIS_process(number_of_task=10,token='AIS')\n",
    "        index = 0\n",
    "        res = []\n",
    "        for i in len_20:\n",
    "            res.append(np.average(y_val[index:index+i]))\n",
    "            index = index+i\n",
    "        y_val = np.array(res)\n",
    "        \n",
    "        temp_zip = list(zip(x_train,y_train))\n",
    "        random.shuffle(temp_zip)\n",
    "        x_train,y_train = zip(*temp_zip)\n",
    "        \n",
    "        \n",
    "        temp_x_trains = x_train[:int(len(x_train)/32)*32]\n",
    "        temp_x_trains = np.reshape(temp_x_trains,[-1,32,200])\n",
    "        x_trains += list(temp_x_trains) + list(x_train[int(len(x_train)/32)*32:])\n",
    "        \n",
    "        temp_y_trains = y_train[:int(len(y_train)/32)*32]\n",
    "        temp_y_trains = np.reshape(temp_y_trains,[-1,32])\n",
    "        tmp = list(temp_y_trains) + list(y_train[int(len(y_train)/32)*32:])\n",
    "        y_trains += tmp\n",
    "        \n",
    "        \n",
    "        \n",
    "        task_labels+=[index1]*len(tmp)\n",
    "        \n",
    "        \n",
    "        y_vals.append(y_val)\n",
    "        x_vals.append(x_val)\n",
    "        len_20s.append(len_20)\n",
    "        val_task.append(index1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "temp_zip = list(zip(x_trains,y_trains,task_labels))\n",
    "random.shuffle(temp_zip)\n",
    "\n",
    "x_trains,y_trains,task_labels = zip(*temp_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15350"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Tox_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,    1,\n",
       "         20,   11,    9,   50,   19,   37,   20,    9,   97,   17,  193,\n",
       "         19,   37,   37,   37,   11,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "Steps : 1460 Task : 0, Loss: 0.7210, acc : 0.8317, auc : nan     Task : 1, Loss: 0.6339, acc : 0.7188, auc : 0.8107     Task : 2, Loss: 0.6988, acc : 0.9281, auc : 0.5363     Task : 3, Loss: 0.5570, acc : 0.7937, auc : 0.8234     Task : 4, Loss: 0.7507, acc : 0.4062, auc : 0.3354     Task : 5, Loss: 0.7151, acc : 0.5688, auc : 0.4020     Task : 6, Loss: 0.6294, acc : 0.6406, auc : 0.7644"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn_reg = tf.keras.losses.MeanSquaredError()\n",
    "acc = np.array([0])\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    loss_list = [[] for i in range(len(Tox_names))]\n",
    "    acc_list = [[] for i in range(len(Tox_names))]\n",
    "    auc_list = [[] for i in range(len(Tox_names))]\n",
    "    for i in range(0, len(temp_zip)):\n",
    "        batch_images = x_trains[i]\n",
    "        batch_images = tf.reshape(batch_images,[-1,200])\n",
    "        batch_labels = y_trains[i]\n",
    "        batch_labels = tf.reshape(batch_labels,[-1])\n",
    "        task = task_labels[i]\n",
    "\n",
    "        if task != 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "                model = globals()[f'Task{task}_model']\n",
    "                logits = model(batch_images)\n",
    "                logits = tf.reshape(logits,[-1])\n",
    "                loss_value = loss_fn(batch_labels, logits)\n",
    "                acc = tf.keras.metrics.Accuracy()(np.round(logits),batch_labels)\n",
    "                auc = tf.keras.metrics.AUC()(batch_labels,logits)\n",
    "                auc = auc.numpy()\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "                loss_list[task].append(loss_value)\n",
    "                acc_list[task].append(acc)\n",
    "                auc_list[task].append(auc)\n",
    "                \n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        else:\n",
    "            with tf.GradientTape() as tape:\n",
    "                model = globals()[f'Task{task}_model']\n",
    "                logits = model(batch_images)\n",
    "                logits = tf.reshape(logits,[-1])\n",
    "                loss_value = loss_fn_reg(logits,batch_labels)\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "                loss_list[task].append(loss_value)\n",
    "                acc = tf.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None)(logits,batch_labels)\n",
    "                acc_list[task].append(acc)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        for l in range(len(Tox_names)):\n",
    "            \n",
    "            temp_loss = np.average(loss_list[l][-10:])\n",
    "            temp_acc = np.average(acc_list[l][-10:])\n",
    "            temp_auc = np.average(auc_list[l][-10:])\n",
    "            if l == 0:\n",
    "                text = \"\\rSteps : {} Task : {}, Loss: {:.4f}, acc : {:.4f}, auc : {:.4f}\".format(i,l,temp_loss,temp_acc,temp_auc)\n",
    "            else:\n",
    "                text += \"     Task : {}, Loss: {:.4f}, acc : {:.4f}, auc : {:.4f}\".format(l,temp_loss,temp_acc,temp_auc)\n",
    "            sys.stdout.write(text)\n",
    "            sys.stdout.flush()\n",
    "    # 각 에포크 종료 후 평가\n",
    "    for temp_index,j in enumerate(val_task):\n",
    "        j = j\n",
    "        val_res = predict(globals()[f'Task{j}_model'],x_vals[temp_index],len_20s[temp_index])\n",
    "        if j != 0:\n",
    "            acc = tf.keras.metrics.Accuracy()(y_vals[temp_index],np.round(val_res))\n",
    "            auc_res = (AUC()(y_vals[temp_index],val_res)).numpy()\n",
    "            loss = tf.keras.metrics.BinaryCrossentropy()(y_vals[temp_index],val_res)\n",
    "            \n",
    "            print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "            print(f\"Test accuracy: {acc}\")\n",
    "            print(f\"Test AUC: {auc_res}\")\n",
    "            print(f\"Test loss: {loss}\\n\")\n",
    "        else:\n",
    "            loss = tf.keras.metrics.MeanSquaredError()(y_vals[temp_index],val_res)\n",
    "            print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "            print(f\"\\nTest MSE:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5132855>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(batch_labels, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([9.9980849e-01, 9.9999833e-01, 9.9994910e-01, 2.6586196e-07,\n",
       "       4.6071282e-04, 9.9972433e-01, 2.5018396e-08, 9.9790084e-01,\n",
       "       9.9999797e-01, 9.9999857e-01, 1.6916485e-07, 9.9999988e-01,\n",
       "       9.9999177e-01, 7.8884675e-04, 9.9999893e-01, 6.5154715e-08,\n",
       "       3.8722131e-08, 8.3348788e-02, 1.0000000e+00, 3.4719207e-07,\n",
       "       1.3504310e-09, 1.0000000e+00, 1.0497830e-02, 9.9999988e-01,\n",
       "       1.6620055e-05, 9.9997461e-01, 9.9999726e-01, 1.3299459e-07,\n",
       "       1.0000000e+00, 3.9634336e-04, 8.8064539e-01, 1.0000000e+00],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([9.9980849e-01, 9.9999833e-01, 9.9994910e-01, 2.6586196e-07,\n",
       "       4.6071282e-04, 9.9972433e-01, 2.5018396e-08, 9.9790084e-01,\n",
       "       9.9999797e-01, 9.9999857e-01, 1.6916485e-07, 9.9999988e-01,\n",
       "       9.9999177e-01, 7.8884675e-04, 9.9999893e-01, 6.5154715e-08,\n",
       "       3.8722131e-08, 8.3348788e-02, 1.0000000e+00, 3.4719207e-07,\n",
       "       1.3504310e-09, 1.0000000e+00, 1.0497830e-02, 9.9999988e-01,\n",
       "       1.6620055e-05, 9.9997461e-01, 9.9999726e-01, 1.3299459e-07,\n",
       "       1.0000000e+00, 3.9634336e-04, 8.8064539e-01, 1.0000000e+00],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task is \n",
      "Test accuracy: 0.821052610874176\n",
      "Test AUC: 0.8632075786590576\n",
      "Test loss: 0.666398286819458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp_index,j in enumerate(val_task):\n",
    "    j = j\n",
    "    val_res = predict(globals()[f'Task{j}_model'],x_vals[temp_index],len_20s[temp_index])\n",
    "    if j != 0:\n",
    "        acc = tf.keras.metrics.Accuracy()(y_vals[temp_index],np.round(val_res))\n",
    "        auc_res = (AUC()(y_vals[temp_index],val_res)).numpy()\n",
    "        loss = tf.keras.metrics.BinaryCrossentropy()(y_vals[temp_index],val_res)\n",
    "        \n",
    "        print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "        print(f\"Test accuracy: {acc}\")\n",
    "        print(f\"Test AUC: {auc_res}\")\n",
    "        print(f\"Test loss: {loss}\\n\")\n",
    "    else:\n",
    "        loss = tf.keras.metrics.MeanSquaredError()(y_vals[temp_index],val_res)\n",
    "        print(f'\\nTask is {Tox_names[temp_index]}')\n",
    "        print(f\"\\nTest MSE:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
