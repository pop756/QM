{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "f = pd.read_excel('Mimicus_toxicity_dataset_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "## Dummy\n",
    "from tensorflow import keras\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlist_physical_devices()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    " tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8644790677490432354\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index_to_word = {}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value+3] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index] = token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, tox_info = f.SMILES,f.toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tox_info.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<sos>', 'big', 'hair', 'big', 'boobs', 'bad', 'music', 'and', 'a', 'giant', 'safety', 'pin', 'these', 'are', 'the', 'words', 'to', 'best', 'describe', 'this', 'terrible', 'movie', 'i', 'love', 'cheesy', 'horror', 'movies', 'and', \"i've\", 'seen', 'hundreds', 'but', 'this', 'had', 'got', 'to', 'be', 'on', 'of', 'the', 'worst', 'ever', 'made', 'the', 'plot', 'is', 'paper', 'thin', 'and', 'ridiculous', 'the', 'acting', 'is', 'an', 'abomination', 'the', 'script', 'is', 'completely', 'laughable', 'the', 'best', 'is', 'the', 'end', 'showdown', 'with', 'the', 'cop', 'and', 'how', 'he', 'worked', 'out', 'who', 'the', 'killer', 'is', \"it's\", 'just', 'so', 'damn', 'terribly', 'written', 'the', 'clothes', 'are', 'sickening', 'and', 'funny', 'in', 'equal', 'measures', 'the', 'hair', 'is', 'big', 'lots', 'of', 'boobs', 'bounce', 'men', 'wear', 'those', 'cut', 'tee', 'shirts', 'that', 'show', 'off', 'their', '<unk>', 'sickening', 'that', 'men', 'actually', 'wore', 'them', 'and', 'the', 'music', 'is', 'just', '<unk>', 'trash', 'that', 'plays', 'over', 'and', 'over', 'again', 'in', 'almost', 'every', 'scene', 'there', 'is', 'trashy', 'music', 'boobs', 'and', '<unk>', 'taking', 'away', 'bodies', 'and', 'the', 'gym', 'still', \"doesn't\", 'close', 'for', '<unk>', 'all', 'joking', 'aside', 'this', 'is', 'a', 'truly', 'bad', 'film', 'whose', 'only', 'charm', 'is', 'to', 'look', 'back', 'on', 'the', 'disaster', 'that', 'was', 'the', \"80's\", 'and', 'have', 'a', 'good', 'old', 'laugh', 'at', 'how', 'bad', 'everything', 'was', 'back', 'then']\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "\n",
    "for i in x_train:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(index_to_word[j])\n",
    "    train_set.append(temp)\n",
    "print(train_set[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RDK as rk\n",
    "from TDC.tdc.single_pred import Tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TDC.tdc.utils import retrieve_label_name_list\n",
    "label_list = retrieve_label_name_list('herg_central')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hERG_at_1uM', 'hERG_at_10uM', 'hERG_inhib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "TD,T = Tox(name = 'herg_central',label_name=label_list[0]).get_data(format = 'DeepPurpose')\n",
    "TF,T1 = Tox(name = 'dili').get_data(format = 'DeepPurpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LNG = np.array(list(TD)+list(TF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_LNG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m A \u001b[39m=\u001b[39m rk\u001b[39m.\u001b[39msmile_tokenize(train_LNG)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m LNG_model \u001b[39m=\u001b[39m rk\u001b[39m.\u001b[39membeding_model(A,vector_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,min_count\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_LNG' is not defined"
     ]
    }
   ],
   "source": [
    "A = rk.smile_tokenize(train_LNG)\n",
    "LNG_model = rk.embeding_model(A,vector_size=256,min_count=5,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_set\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from TDC.tdc.single_pred import Tox\n",
    "train,tox_info = Tox(name = 'hERG_Karim' ).get_data(format = 'DeepPurpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full size :  13445\n"
     ]
    }
   ],
   "source": [
    "print('full size : ',len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:15000]\n",
    "tox_info = tox_info[:15000]\n",
    "train_set = rk.smile_tokenize(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 중\n",
      "Loss after epoch 0: 9528687.0\n",
      "Loss after epoch 1: 8105403.0\n",
      "Loss after epoch 2: 7394978.0\n",
      "Loss after epoch 3: 6522096.0\n",
      "Loss after epoch 4: 5199276.0\n",
      "Loss after epoch 5: 4313392.0\n",
      "Loss after epoch 6: 4248928.0\n",
      "Loss after epoch 7: 4285248.0\n",
      "Loss after epoch 8: 4257516.0\n",
      "Loss after epoch 9: 4358156.0\n",
      "Loss after epoch 10: 4183116.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mRDK\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mrk\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m LNG_small \u001b[39m=\u001b[39m rk\u001b[39m.\u001b[39;49membeding_model(train_set,vector_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,min_count\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\RDK.py:118\u001b[0m, in \u001b[0;36membeding_model\u001b[1;34m(corpus, vector_size, workers, sg, compute_loss, epochs, min_count)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m학습 중\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(corpus, vector_size\u001b[39m=\u001b[39;49mvector_size, workers\u001b[39m=\u001b[39;49mworkers, sg\u001b[39m=\u001b[39;49msg, compute_loss\u001b[39m=\u001b[39;49mcompute_loss, epochs\u001b[39m=\u001b[39;49mepochs,min_count\u001b[39m=\u001b[39;49mmin_count, callbacks\u001b[39m=\u001b[39;49m[callback()])\n\u001b[0;32m    119\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m완료\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[39m.\u001b[39mon_epoch_begin(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(\n\u001b[0;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[0;32m   1075\u001b[0m         total_words\u001b[39m=\u001b[39;49mtotal_words, queue_factor\u001b[39m=\u001b[39;49mqueue_factor, report_delay\u001b[39m=\u001b[39;49mreport_delay,\n\u001b[0;32m   1076\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1077\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[39m=\u001b[39mcur_epoch, total_examples\u001b[39m=\u001b[39mtotal_examples, total_words\u001b[39m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_epoch_progress(\n\u001b[0;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[0;32m   1436\u001b[0m     total_words\u001b[39m=\u001b[39;49mtotal_words, report_delay\u001b[39m=\u001b[39;49mreport_delay, is_corpus_file_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1437\u001b[0m )\n\u001b[0;32m   1439\u001b[0m \u001b[39mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[39mwhile\u001b[39;00m unfinished_worker_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[39m=\u001b[39m progress_queue\u001b[39m.\u001b[39;49mget()  \u001b[39m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[39mif\u001b[39;00m report \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[0;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import RDK as rk\n",
    "LNG_small = rk.embeding_model(train_set,vector_size=256,min_count=5,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size :  25000\n",
      "word number :  19606\n"
     ]
    }
   ],
   "source": [
    "print('train size : ',len(train_set))\n",
    "print('word number : ',len(LNG_small.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Practice/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(file_name,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "\n",
    "for i in train_set[:5000]:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        try:\n",
    "            temp.append(word2vec[j])\n",
    "        except:\n",
    "            temp.append([0]*300)\n",
    "    image.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 200, 300)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(rk.string_to_vec(LNG_small,train_set[:5000],200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "def scaled_dot_product(Q,K,V):\n",
    "        mat_QK = tf.matmul(Q,K,transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        logits = mat_QK / tf.math.sqrt(depth)\n",
    "    \n",
    "\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "class self_multihead(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,name='multiheadatten'):\n",
    "        super(self_multihead,self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model%num_heads == 0\n",
    "\n",
    "        self.depth = d_model//self.num_heads\n",
    "        self.Q = Dense(units=d_model)\n",
    "        self.K = Dense(units=d_model)\n",
    "        self.V = Dense(units=d_model)\n",
    "\n",
    "        self.linear = Dense(units = d_model)\n",
    "\n",
    "    def split(self,inputs,batch_size):\n",
    "        inputs = tf.reshape(inputs, [batch_size,-1,self.num_heads,self.depth])\n",
    "        return tf.transpose(inputs,perm=[0,2,1,3])\n",
    "\n",
    "    def call(self,inputs):\n",
    "        Q,K,V = inputs,inputs,inputs\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        \n",
    "        # Denselayer\n",
    "        Q = self.Q(Q)\n",
    "        K = self.K(K)\n",
    "        V = self.V(V)\n",
    "        #Splithead \n",
    "\n",
    "        Q = self.split(Q,batch_size)\n",
    "        K = self.split(K,batch_size)\n",
    "        V = self.split(V,batch_size)\n",
    "        #Scaled dot product\n",
    "        scaled_att,_ = scaled_dot_product(Q,K,V)\n",
    "        #(batchsize,sentence_size,num_heads,word_size/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_att, perm=[0,2,1,3])\n",
    "        #Concate\n",
    "        concat_attention = tf.reshape(scaled_attention, [batch_size, -1, self.d_model])\n",
    "        #Linear Dense\n",
    "        outputs = self.linear(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def scheduler(epochs,lr):\n",
    "    lr = 0.5*(1+np.cos(np.pi * epochs / 100))\n",
    "    if epochs <= 5:\n",
    "        lr *= epochs * 1.0 / 30\n",
    "    return lr\n",
    "\n",
    "class transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,ff_dim,name):\n",
    "        super(transformer,self).__init__(name=name)\n",
    "        self.attn = self_multihead(d_model,num_heads)\n",
    "        self.ff = Sequential([Dense(ff_dim,activation='relu'),Dense(d_model)])\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        attn_out = self.attn(inputs)\n",
    "        out1 = self.norm1(inputs + attn_out)\n",
    "        ffn_output = self.ff(out1)\n",
    "        return self.norm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "temp_image = []\n",
    "temp_tox_image = []\n",
    "for i in image:\n",
    "    if np.sum(i) == 0:\n",
    "        print(np.sum(i))\n",
    "        print(index)\n",
    "    else:\n",
    "        temp_image.append(i)\n",
    "        temp_tox_image.append(tox_info[index])\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 200, 300)]        0         \n",
      "                                                                 \n",
      " tf.math.add_1 (TFOpLambda)  (None, 200, 300)          0         \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 200, 300)          1031312   \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 300)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1091713 (4.16 MB)\n",
      "Trainable params: 1091713 (4.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "pos = get_sinusoid_encoding_table(200,300)\n",
    "inputs = Input(shape=(200,300,))\n",
    "#inputs = LayerNormalization()(inputs)\n",
    "#pos = tf.range(start=0, limit=55, delta=1)\n",
    "#pos = Embedding(55, 300)(pos)\n",
    "x = inputs +pos\n",
    "\n",
    "\n",
    "hidden7 = TransformerBlock(300,2,512)(x)\n",
    "\n",
    "\n",
    "\n",
    "outputs = GlobalMaxPooling1D()(hidden7)\n",
    "outputs = Dense(200,activation = 'relu')(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(1,activation = 'sigmoid')(outputs)\n",
    "model = Model(inputs= [inputs], outputs=[outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 200, 300)]        0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 1)                 321401    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321401 (1.23 MB)\n",
      "Trainable params: 321401 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "#pos = get_sinusoid_encoding_table(200,300)\n",
    "inputs = Input(shape=(200,300,))\n",
    "x = inputs\n",
    "outputs = Sequential([\n",
    "    layers.LSTM(100, return_sequences=True, input_shape=(x.shape[1], x.shape[2])),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Bidirectional(layers.LSTM(100)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])(x)\n",
    "LSTM_model = Model(inputs= [inputs], outputs=[outputs])\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TokenAndPositionEmbedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 34\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ff_dim \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m  \u001b[39m# Hidden layer size in feed forward network inside transformer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m inputs \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(maxlen,))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m embedding_layer \u001b[39m=\u001b[39m TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m embedding_layer(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m transformer_block \u001b[39m=\u001b[39m TransformerBlock(embed_dim, num_heads, ff_dim)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TokenAndPositionEmbedding' is not defined"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_val,y_train,y_val = train_test_split(np.array(temp_image),np.array(temp_tox_image)/1.,test_size=0.2,random_state=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_val,y_train,y_val = train_test_split(image,y_train[:5000]/1.,test_size=0.2,random_state=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val = LayerNormalization()(x_train),LayerNormalization()(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "early_stopping_cb = EarlyStopping(patience=3, monitor='val_loss',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(),loss = 'binary_crossentropy',metrics=['acc'])\n",
    "LSTM_model.compile(optimizer=Adam(),loss = 'binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m LSTM_model\u001b[39m.\u001b[39;49mfit(x_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(x_val,y_val),batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py:1721\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cluster_coordinator \u001b[39m=\u001b[39m (\n\u001b[0;32m   1712\u001b[0m         tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mcoordinator\u001b[39m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1713\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\n\u001b[0;32m   1714\u001b[0m         )\n\u001b[0;32m   1715\u001b[0m     )\n\u001b[0;32m   1717\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1718\u001b[0m     \u001b[39mself\u001b[39m\n\u001b[0;32m   1719\u001b[0m ):\n\u001b[0;32m   1720\u001b[0m     \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1721\u001b[0m     data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1722\u001b[0m         x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1723\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1724\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1725\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1726\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1727\u001b[0m         initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   1728\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   1729\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1730\u001b[0m         class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   1731\u001b[0m         max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1732\u001b[0m         workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1733\u001b[0m         use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1734\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1735\u001b[0m         steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   1736\u001b[0m     )\n\u001b[0;32m   1738\u001b[0m     \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1687\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1688\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1291\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m-> 1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1293\u001b[0m     x,\n\u001b[0;32m   1294\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39mpss_evaluation_shards,\n\u001b[0;32m   1306\u001b[0m )\n\u001b[0;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1102\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_data_adapter\u001b[39m(x, y):\n\u001b[0;32m   1101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Selects a data adapter that can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1102\u001b[0m     adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39;49m \u001b[39mfor\u001b[39;49;00m \u001b[39mcls\u001b[39;49m \u001b[39min\u001b[39;49;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;49;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m         \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m                 _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m             )\n\u001b[0;32m   1109\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1102\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_data_adapter\u001b[39m(x, y):\n\u001b[0;32m   1101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Selects a data adapter that can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1102\u001b[0m     adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m         \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m                 _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m             )\n\u001b[0;32m   1109\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:614\u001b[0m, in \u001b[0;36mCompositeTensorDataAdapter.can_handle\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m _is_composite(v)\n\u001b[1;32m--> 614\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39;49m(_is_composite(v) \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m flat_inputs) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    615\u001b[0m     _is_tensor_or_composite(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m flat_inputs\n\u001b[0;32m    616\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:614\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m _is_composite(v)\n\u001b[1;32m--> 614\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(_is_composite(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m flat_inputs) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    615\u001b[0m     _is_tensor_or_composite(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m flat_inputs\n\u001b[0;32m    616\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:601\u001b[0m, in \u001b[0;36mCompositeTensorDataAdapter.can_handle.<locals>._is_composite\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_composite\u001b[39m(v):\n\u001b[0;32m    598\u001b[0m     \u001b[39m# Dataset/iterator/DistributedDataset inherits from CompositeTensor\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     \u001b[39m# but should be handled by DatasetAdapter and GeneratorAdapter.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m--> 601\u001b[0m         tf_utils\u001b[39m.\u001b[39;49mis_extension_type(v)\n\u001b[0;32m    602\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(v, (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mIterator))\n\u001b[0;32m    603\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_distributed_dataset(v)\n\u001b[0;32m    604\u001b[0m     ):\n\u001b[0;32m    605\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    606\u001b[0m     \u001b[39m# Support Scipy sparse tensors if scipy is installed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:401\u001b[0m, in \u001b[0;36mis_extension_type\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_type\u001b[39m(tensor):\n\u001b[0;32m    388\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns whether a tensor is of an ExtensionType.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \n\u001b[0;32m    390\u001b[0m \u001b[39m    github.com/tensorflow/community/pull/269\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39m      True if the tensor is an extension type object, false if not.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mCompositeTensor)\n",
      "File \u001b[1;32m<frozen abc>:117\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = LSTM_model.fit(x_train,y_train,epochs=100,validation_data=(x_val,y_val),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model = model.fit(x_train,y_train,epochs=100,validation_data=(x_val,y_val),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(x_val,y_val)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_string = rk.smile_to_RDkit(train,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_157\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 2048)]            0         \n",
      "                                                                 \n",
      " dense_970 (Dense)           (None, 250)               512250    \n",
      "                                                                 \n",
      " dropout_465 (Dropout)       (None, 250)               0         \n",
      "                                                                 \n",
      " dense_971 (Dense)           (None, 40)                10040     \n",
      "                                                                 \n",
      " dropout_466 (Dropout)       (None, 40)                0         \n",
      "                                                                 \n",
      " dense_972 (Dense)           (None, 10)                410       \n",
      "                                                                 \n",
      " dropout_467 (Dropout)       (None, 10)                0         \n",
      "                                                                 \n",
      " dense_973 (Dense)           (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 522711 (1.99 MB)\n",
      "Trainable params: 522711 (1.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "inputs = Input(shape=(2048,),name='input')\n",
    "outputs = Dense(250,activation = 'relu')(inputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(40,activation = 'relu')(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(10,activation = 'relu')(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(1,activation = 'sigmoid')(outputs)\n",
    "\n",
    "model_NN = Model(inputs= [inputs], outputs=[outputs])\n",
    "model_NN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5000, 1181]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_train,x_val,y_train,y_val \u001b[39m=\u001b[39m train_test_split(np\u001b[39m.\u001b[39;49marray(bit_string)\u001b[39m/\u001b[39;49m\u001b[39m1.\u001b[39;49m,np\u001b[39m.\u001b[39;49marray(tox_info)\u001b[39m/\u001b[39;49m\u001b[39m1.\u001b[39;49m,test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,random_state\u001b[39m=\u001b[39;49m\u001b[39m153\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2614\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2611\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2612\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\sklearn\\utils\\validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 455\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    456\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5000, 1181]"
     ]
    }
   ],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(np.array(bit_string)/1.,np.array(tox_info)/1.,test_size=0.2,random_state=153)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss = 'binary_crossentropy',optimizer=Adam(),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_157\" is incompatible with the layer: expected shape=(None, 2048), found shape=(None, 50, 256)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Qunova\\Qunova code\\SMILE_to_vec.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pop75/OneDrive/Desktop/Qunova/Qunova%20code/SMILE_to_vec.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model1\u001b[39m.\u001b[39;49mfit(x_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(x_val,y_val),batch_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9wyf3q0e.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_157\" is incompatible with the layer: expected shape=(None, 2048), found shape=(None, 50, 256)\n"
     ]
    }
   ],
   "source": [
    "model1.fit(x_train,y_train,epochs=50,validation_data=(x_val,y_val),batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 11ms/step - loss: 1.4291 - acc: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4291437864303589, 0.7749999761581421]"
      ]
     },
     "execution_count": 1092,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
