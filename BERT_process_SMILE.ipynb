{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 22:44:07.286663: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-23 22:44:07.434030: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-23 22:44:08.264343: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2023-12-23 22:44:08.264441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.3/lib64\n",
      "2023-12-23 22:44:08.264451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 22:44:09.155051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-23 22:44:10.149861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78900 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2023-12-23 22:44:10.169217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78900 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "\n",
    "    # 메모리 40% 할당\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Pretraning_data/ZINC_10M_data','rb') as file:\n",
    "    train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmilesPE import tokenizer\n",
    "\n",
    "tokenizer.atomwise_tokenizer(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "def Chem_generator(smiles):\n",
    "    res_list = []\n",
    "    for i in tqdm(smiles):\n",
    "        mol = Chem.MolFromSmiles(i)\n",
    "        temp = []\n",
    "        index = 0\n",
    "        while(len(set(temp))!=4 and index != 100):\n",
    "            index+=1\n",
    "            temp.append(Chem.MolToSmiles(mol,doRandom=True))\n",
    "        res_list+=list(set(temp))\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 432/12525050 [00:00<1:37:37, 2138.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12525050/12525050 [1:14:21<00:00, 2807.07it/s]\n"
     ]
    }
   ],
   "source": [
    "train_1 = Chem_generator(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.single_pred import Tox\n",
    "from Module import RDK as rk\n",
    "import atomInSmiles\n",
    "from Module import Fine_tune\n",
    "from SmilesPE import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50100189 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50100189/50100189 [08:07<00:00, 102816.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, Value, Array\n",
    "from multiprocessing import Process,Manager,current_process\n",
    "from Module import RDK as rk\n",
    "import atomInSmiles \n",
    "from SmilesPE import tokenizer\n",
    "\n",
    "train_set = []\n",
    "for i in tqdm(train_1):\n",
    "    train_set.append(tokenizer.atomwise_tokenizer(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "length_list = {}\n",
    "\n",
    "\n",
    "for index,i in enumerate(train_set_plus):\n",
    "    try:\n",
    "        length_list[len(i)]=length_list[len(i)]+1\n",
    "    except:\n",
    "        length_list[len(i)] = 1\n",
    "    \n",
    "plt.bar(length_list.keys(),length_list.values())\n",
    "plt.xlabel(xlabel='Token_size')\n",
    "plt.ylabel('counts')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P@H+] 48\n",
      "[P@@H+] 48\n",
      "9 12\n",
      "%10 4\n",
      "[CH] 16\n",
      "p 16\n",
      "[CH2] 24\n",
      "[Si-] 4\n",
      "[P@+] 33\n",
      "[P@@+] 31\n",
      "[NH] 8\n",
      "[17O] 12\n",
      "[p+] 4\n"
     ]
    }
   ],
   "source": [
    "temp_dict = {}\n",
    "for i in train_set:\n",
    "    for j in i:\n",
    "        try:\n",
    "            temp_dict[j] = temp_dict[j] + 1\n",
    "        except:\n",
    "            temp_dict[j] = 1\n",
    "remove_dict = {}\n",
    "for i in temp_dict.keys():\n",
    "    if temp_dict[i]<50:\n",
    "        print(i,temp_dict[i])\n",
    "        remove_dict[i] = 1\n",
    "\n",
    "remove_list = []\n",
    "for index,i in enumerate(train_set):\n",
    "    for j in i:\n",
    "        try:\n",
    "            remove_dict[j]\n",
    "            remove_list.append(index)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "remove_list.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./Pretraning_data/Random_ZINC_50M_SMILE_tokken.pkl','rb') as file:\n",
    "    train_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "molecule_dictionary = {'<start>':1,'<end>':2,'<unknown1>':3,'<unknown2>':4,'<unknown3>':5,'<unknow4>':6,'<unknown5>':7}\n",
    "\n",
    "for molecule in train_set:\n",
    "    for atom in molecule:\n",
    "        try:\n",
    "            molecule_dictionary[atom]\n",
    "        except:\n",
    "            molecule_dictionary[atom] = len(molecule_dictionary)+1\n",
    "with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','wb') as file:\n",
    "    pickle.dump(molecule_dictionary,file)\n",
    "len(molecule_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','rb') as file:\n",
    "    molecule_dictionary = pickle.load(file)\n",
    "len(molecule_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50099943/50099943 [13:47<00:00, 60572.57it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "\n",
    "def word_to_index(train_set,dict):\n",
    "    result = []\n",
    "    for molecule in tqdm(train_set):\n",
    "        temp_list = []\n",
    "        temp_list.append(1)\n",
    "        for atom in molecule:\n",
    "            temp_list.append(dict[atom])\n",
    "        while len(temp_list)!=200:\n",
    "            temp_list.append(0)\n",
    "        result.append(temp_list)\n",
    "    return result\n",
    "embedding_word = word_to_index(train_set,molecule_dictionary)   \n",
    "embedding_word = np.array(embedding_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6188058/6188058 [01:35<00:00, 65024.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "\n",
    "def word_to_index(train_set,dict):\n",
    "    result = []\n",
    "    for molecule in tqdm(train_set):\n",
    "        temp_list = []\n",
    "        temp_list.append(1)\n",
    "        for atom in molecule:\n",
    "            temp_list.append(dict[atom])\n",
    "        while len(temp_list)!=200:\n",
    "            temp_list.append(0)\n",
    "        result.append(temp_list)\n",
    "    return result\n",
    "embedding_word = word_to_index(train_set_plus,molecule_dictionary)   \n",
    "embedding_word = np.array(embedding_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2834.80it/s]\n",
      "100%|██████████| 19937/19937 [00:00<00:00, 60741.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tdc.single_pred import Tox\n",
    "from Module import RDK\n",
    "import atomInSmiles\n",
    "\n",
    "def word_to_index(train_set,dict):\n",
    "    result = []\n",
    "    for molecule in tqdm(train_set):\n",
    "        temp_list = []\n",
    "        temp_list.append(1)\n",
    "        for atom in molecule:\n",
    "            temp_list.append(dict[atom])\n",
    "        while len(temp_list)!=200:\n",
    "            temp_list.append(0)\n",
    "        result.append(temp_list)\n",
    "    return result\n",
    "train_val,tox_info = Tox(name = 'AMES').get_data(format = 'DeepPurpose')\n",
    "train_set_val = []\n",
    "\n",
    "train_val = Chem_generator(train_val[:5000])\n",
    "\n",
    "\n",
    "for i in train_val:\n",
    "    train_set_val.append(tokenizer.atomwise_tokenizer(i))\n",
    "val_remove_list = []\n",
    "for index,i in enumerate(train_set_val):\n",
    "    for j in i:\n",
    "        try:\n",
    "            molecule_dictionary[j]\n",
    "        except:\n",
    "            val_remove_list.append(index)\n",
    "            break\n",
    "val_remove_list.sort(reverse=True)\n",
    "\n",
    "for i in val_remove_list:\n",
    "    train_set_val.pop(i)\n",
    "\n",
    "embedding_word_val = word_to_index(train_set_val,molecule_dictionary)   \n",
    "embedding_word_val = np.array(embedding_word_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 11, 10, ...,  0,  0,  0],\n",
       "       [ 1, 20, 19, ...,  0,  0,  0],\n",
       "       [ 1, 11, 10, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1,  8, 10, ...,  0,  0,  0],\n",
       "       [ 1, 29, 11, ...,  0,  0,  0],\n",
       "       [ 1,  8, 10, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_word_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19937/19937 [00:00<00:00, 205042.93it/s]\n",
      "2023-12-23 22:44:50.443027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78900 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "max = 16\n",
    "mask_input_val = []\n",
    "for j in tqdm(train_set_val):\n",
    "        value = []\n",
    "        number = int(len(j)*0.15)\n",
    "        if number>max:\n",
    "                max = number\n",
    "        if number == 0:\n",
    "                number = 1\n",
    "        value += random.sample(range(1,len(j)),number)\n",
    "        mask_input_val.append(value)\n",
    "        \n",
    "        \n",
    "for j in mask_input_val:\n",
    "        while(len(j)<max):\n",
    "                j.append(-1)\n",
    "\n",
    "\n",
    "random_value_val = embedding_word_val.copy()        \n",
    "for _,index in enumerate(mask_input_val):\n",
    "        for j in index:\n",
    "                if j != -1:\n",
    "                        prob = np.random.rand(1)[0]\n",
    "                        if prob < 0.8:\n",
    "                                random_value_val[_][j] = 5\n",
    "                        elif prob > 0.9:\n",
    "                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                random_value_val[_][j] = temp1\n",
    "                                \n",
    "output_val = tf.multiply(tf.reduce_sum(tf.one_hot(mask_input_val,200),axis=1),embedding_word_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import *\n",
    "early_stopping_cb = EarlyStopping(patience=6, monitor='loss',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    output = tf.one_hot(tf.cast(tf.boolean_mask(y_true,tf.cast(y_true,bool)),tf.int32),121)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()(output,y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_metric(y_true,y_pred):\n",
    "    predictions = tf.argmax(y_pred,axis=1)\n",
    "    true = tf.boolean_mask(y_true,tf.cast(y_true,bool))\n",
    "    return  tf.metrics.Accuracy()(predictions,true)\n",
    "def Mask_acc(y_true, y_pred):\n",
    "    score = tf.py_function(func=Custom_metric, inp=[y_true, y_pred], Tout=tf.float32,  name='Custom_acc') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        self.transformer = TransformerBlock(emb_dim,num_heads,ff_dim)\n",
    "        \n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = layers.Dense(len(molecule_dictionary),activation = 'softmax')\n",
    "    def call(self, inputs, mask_index,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "        \n",
    "        hidden = self.embedding(inputs)\n",
    "        for i in range(8):\n",
    "            hidden = self.transformer(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,mask_zero = True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module import custom_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_tensor = custom_layers.BERT_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " bert_tensor (BERT_tensor)      (None, 121)          11527545    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,527,545\n",
      "Trainable params: 11,527,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "mask = Input(shape = (16), dtype=tf.int32)\n",
    "outputs = BERT_tensor(256,6,1024,len(molecule_dictionary)+50)(inputs,mask,pretrain=True)\n",
    "\n",
    "\n",
    "model = Model(inputs = [inputs,mask], outputs = [outputs])\n",
    "model.summary()\n",
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "model.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=2000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.batch_count\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    self.batch_count += 1\n",
    "    step = float(step+1)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(1000.) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps=4000):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        self.step.assign_add(1)  # 각 배치가 지나갈 때마다 step 증가\n",
    "        lr = tf.cond(\n",
    "            self.step <= self.warmup_steps,\n",
    "            lambda: (1e-4 - 1e-9) / self.warmup_steps * tf.cast(self.step, tf.float32) + 1e-9,\n",
    "            lambda: 0.5 * (1e-4 + 1e-9) * (self.warmup_steps ** 0.5) / (tf.cast(self.step, tf.float32) ** 0.5)\n",
    "        )\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = CustomLearningRateScheduler()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f6cf01ab730>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./BERT/SMILE/small_tensor_Random_ZINC_L_model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     16\u001b[0m mask_input \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mtrain_set\u001b[49m[size\u001b[38;5;241m*\u001b[39mi:size\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]):\n\u001b[1;32m     18\u001b[0m         value \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m         number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(j)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.15\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "size = 300000\n",
    "for k in range(1):\n",
    "        for i in range(0,int(len(embedding_word)/size)):\n",
    "                if k == 0:\n",
    "                        i = i+48\n",
    "                if i == int(len(embedding_word)/size):\n",
    "                        break\n",
    "                if i <80:\n",
    "                        lr = (1e-4-1e-6)*(i)/80 + 1e-6\n",
    "                else:\n",
    "                        lr = 1e-4*np.sqrt(80)/np.sqrt(i)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "                model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)\n",
    "                max = 16\n",
    "                mask_input = []\n",
    "                for j in tqdm(train_set[size*i:size*(i+1)]):\n",
    "                        value = []\n",
    "                        number = int(len(j)*0.15)\n",
    "                        if number>max:\n",
    "                                number = max\n",
    "                        if number == 0:\n",
    "                                number = 1\n",
    "                        value += random.sample(range(1,len(j)),number)\n",
    "                        mask_input.append(value)\n",
    "                        \n",
    "                        \n",
    "                for j in mask_input:\n",
    "                        while(len(j)<max):\n",
    "                                j.append(-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                inputs1 = embedding_word[size*i:size*(i+1)]\n",
    "                inputs2 = mask_input\n",
    "                output = tf.multiply(tf.reduce_sum(tf.one_hot(inputs2,200),axis=1),inputs1)\n",
    "\n",
    "                \n",
    "                random_value = inputs1.copy()        \n",
    "                for _,index in enumerate(inputs2):\n",
    "                        for j in index:\n",
    "                                if j != -1:\n",
    "                                        prob = np.random.rand(1)[0]\n",
    "                                        if prob < 0.8:\n",
    "                                                random_value[_][j] = 5\n",
    "                                        elif prob > 0.9:\n",
    "                                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                                random_value[_][j] = temp1\n",
    "                                                \n",
    "                \n",
    "                \n",
    "                print(f'This is {i} number step')\n",
    "                with tf.device('/device:GPU:0'):\n",
    "                        if i % 10 == 0:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=32,callbacks = [early_stopping_cb],validation_data=([np.array(random_value_val),np.array(mask_input_val)],np.array(output_val)))\n",
    "                        else:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=32,callbacks = [early_stopping_cb])\n",
    "                        print(model.optimizer.lr)\n",
    "                        model.save_weights('./BERT/SMILE/small_tensor_Random_ZINC_L_model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:01<00:00, 196646.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 0 number step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 23:01:37.516977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-23 23:01:37.551099: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f6197ed0e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-23 23:01:37.551130: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2023-12-23 23:01:37.558513: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-23 23:01:37.711546: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 [==============================] - 981s 101ms/step - loss: 0.2101 - Mask_acc: 0.9297 - val_loss: 0.3070 - val_Mask_acc: 0.9002\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:01<00:00, 193963.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 1 number step\n",
      "9375/9375 [==============================] - 911s 94ms/step - loss: 0.1379 - Mask_acc: 0.9517\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:01<00:00, 190842.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 2 number step\n",
      "9182/9375 [============================>.] - ETA: 18s - loss: 0.1340 - Mask_acc: 0.9527"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit([np\u001b[38;5;241m.\u001b[39marray(random_value),np\u001b[38;5;241m.\u001b[39marray(inputs2)],np\u001b[38;5;241m.\u001b[39marray(output,dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m),epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,callbacks \u001b[38;5;241m=\u001b[39m [early_stopping_cb],validation_data\u001b[38;5;241m=\u001b[39m([np\u001b[38;5;241m.\u001b[39marray(random_value_val),np\u001b[38;5;241m.\u001b[39marray(mask_input_val)],np\u001b[38;5;241m.\u001b[39marray(output_val)))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./BERT/SMILE/small_tensor_Random_ZINC_L_model_weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "size = 300000\n",
    "for k in range(1):\n",
    "        for i in range(0,int(len(embedding_word)/size)):\n",
    "                if k == 0:\n",
    "                        i = i\n",
    "                if i == int(len(embedding_word)/size):\n",
    "                        break\n",
    "                if i <80:\n",
    "                        lr = (1e-4-1e-6)*(i)/80 + 1e-6\n",
    "                else:\n",
    "                        lr = 1e-4*np.sqrt(80)/np.sqrt(i)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "                model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)\n",
    "                max = 16\n",
    "                mask_input = []\n",
    "                for j in tqdm(train_set_plus[size*i:size*(i+1)]):\n",
    "                        value = []\n",
    "                        number = int(len(j)*0.15)\n",
    "                        if number>max:\n",
    "                                number = max\n",
    "                        if number == 0:\n",
    "                                number = 1\n",
    "                        value += random.sample(range(1,len(j)),number)\n",
    "                        mask_input.append(value)\n",
    "                        \n",
    "                        \n",
    "                for j in mask_input:\n",
    "                        while(len(j)<max):\n",
    "                                j.append(-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                inputs1 = embedding_word[size*i:size*(i+1)]\n",
    "                inputs2 = mask_input\n",
    "                output = tf.multiply(tf.reduce_sum(tf.one_hot(inputs2,200),axis=1),inputs1)\n",
    "\n",
    "                \n",
    "                random_value = inputs1.copy()        \n",
    "                for _,index in enumerate(inputs2):\n",
    "                        for j in index:\n",
    "                                if j != -1:\n",
    "                                        prob = np.random.rand(1)[0]\n",
    "                                        if prob < 0.8:\n",
    "                                                random_value[_][j] = 5\n",
    "                                        elif prob > 0.9:\n",
    "                                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                                random_value[_][j] = temp1\n",
    "                                                \n",
    "                \n",
    "                \n",
    "                print(f'This is {i} number step')\n",
    "                with tf.device('/device:GPU:0'):\n",
    "                        if i % 10 == 0:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=32,callbacks = [early_stopping_cb],validation_data=([np.array(random_value_val),np.array(mask_input_val)],np.array(output_val)))\n",
    "                        else:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=32,callbacks = [early_stopping_cb])\n",
    "                        print(model.optimizer.lr)\n",
    "                        model.save_weights('./BERT/SMILE/small_tensor_Random_ZINC_L_model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " bert_tensor_1 (BERT_tensor)  (None, 200, 256)         11496448  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,496,448\n",
      "Trainable params: 11,496,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "outputs = BERT_tensor(256,6,1024,len(molecule_dictionary)+50)(inputs,None)\n",
    "\n",
    "\n",
    "model_temp = Model(inputs = [inputs], outputs = [outputs])\n",
    "model_temp.summary()\n",
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "model_temp.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temp.set_weights(paras[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.2450"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 1.2450"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "for i in range(50):\n",
    "    for j in range(1000000):\n",
    "        j = j +1\n",
    "    sys.stdout.write(\"\\rEpoch {}, Loss: {:.4f}\".format(i, 1.245))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temp.save_weights('./BERT/SMILE/small_tensor_Pre_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for molecule in train_set_plus:\n",
    "    for atom in molecule:\n",
    "        try:\n",
    "            molecule_dictionary[atom]\n",
    "        except:\n",
    "            molecule_dictionary[atom] = len(molecule_dictionary)+1\n",
    "with open('./BERT/SMILE/1M_random_ZINC_word2index.pkl','wb') as file:\n",
    "    pickle.dump(molecule_dictionary,file)\n",
    "len(molecule_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weights = model.get_weights()\n",
    "all_weights = temp_weights[:16]*8 + temp_weights[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self,emb_dim,num_heads,ff_dim):\n",
    "        super(BERT, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([TransformerBlock(emb_dim,num_heads,ff_dim) for i in range(8)])\n",
    "        #self.encoder = TransformerBlock(emb_dim,num_heads,ff_dim)\n",
    "        #self.normalize = tf.keras.layers.LayerNormalization(epsilon=1e-8)\n",
    "        \n",
    "        self.embedding = TokenAndPositionEmbedding(200,3500,256)\n",
    "        self.dense = layers.Dense(250,activation = 'gelu')\n",
    "        self.classify = layers.Dense(len(molecule_dictionary),activation = 'softmax')\n",
    "    def call(self, inputs, mask_index,pretrain = False):\n",
    "        if pretrain:\n",
    "            mask_index = tf.one_hot(mask_index,200)\n",
    "            boolean_mask = tf.cast(tf.reduce_sum(mask_index,axis=1),bool)\n",
    "            inputs = tf.cast(inputs,dtype=tf.int32)\n",
    "            \n",
    "        hidden = self.embedding(inputs)\n",
    "        hidden = self.encoder(hidden)\n",
    "    \n",
    "        if pretrain:\n",
    "            output = tf.reshape(hidden,[-1,200,256])\n",
    "            output = self.dense(output)\n",
    "            output = layers.Dropout(0.1)(output)\n",
    "            output = self.classify(output)\n",
    "            output = tf.boolean_mask(output,boolean_mask)\n",
    "            return output\n",
    "        else:\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50094160"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " bert_7 (BERT)                  (None, 71)           17863831    ['input_15[0][0]',               \n",
      "                                                                  'input_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,863,831\n",
      "Trainable params: 17,863,831\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (200,),dtype=tf.int32)\n",
    "mask = Input(shape = (16), dtype=tf.int32)\n",
    "outputs = BERT(256,6,1024)(inputs,mask,pretrain=True)\n",
    "\n",
    "model = Model(inputs = [inputs,mask], outputs = [outputs])\n",
    "model.summary()\n",
    "optmizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "model.compile(optimizer=optmizer,loss = custom_loss,metrics = Mask_acc)\n",
    "model.set_weights(all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_value_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:01<00:00, 186390.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 0 number step\n",
      "1172/1172 [==============================] - 392s 321ms/step - loss: 0.5511 - Mask_acc: 0.8475 - val_loss: 0.9338 - val_Mask_acc: 0.7538\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:01<00:00, 204244.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 1 number step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit([np\u001b[38;5;241m.\u001b[39marray(random_value),np\u001b[38;5;241m.\u001b[39marray(inputs2)],np\u001b[38;5;241m.\u001b[39marray(output,dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m),epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,callbacks \u001b[38;5;241m=\u001b[39m [early_stopping_cb],validation_data\u001b[38;5;241m=\u001b[39m([np\u001b[38;5;241m.\u001b[39marray(random_value_val),np\u001b[38;5;241m.\u001b[39marray(mask_input_val)],np\u001b[38;5;241m.\u001b[39marray(output_val)))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./BERT/SMILE/Random_ZINC_L_model_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:945\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    948\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    949\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    950\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "size = 300000\n",
    "for k in range(11):\n",
    "        for i in range(0,int(len(embedding_word)/size)):\n",
    "                if k == 1:\n",
    "                        i = i\n",
    "                if i == int(len(embedding_word)/size):\n",
    "                        break\n",
    "                if i <80:\n",
    "                        lr = (1e-4-1e-6)*(i)/80 + 1e-6\n",
    "                else:\n",
    "                        lr = 1e-4*np.sqrt(80)/np.sqrt(i)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "                model.compile(optimizer= optimizer, loss = custom_loss, metrics= Mask_acc)\n",
    "                max = 16\n",
    "                mask_input = []\n",
    "                for j in tqdm(train_set[size*i:size*(i+1)]):\n",
    "                        value = []\n",
    "                        number = int(len(j)*0.15)\n",
    "                        if number>max:\n",
    "                                number = max\n",
    "                        if number == 0:\n",
    "                                number = 1\n",
    "                        value += random.sample(range(1,len(j)),number)\n",
    "                        mask_input.append(value)\n",
    "                        \n",
    "                        \n",
    "                for j in mask_input:\n",
    "                        while(len(j)<max):\n",
    "                                j.append(-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                inputs1 = embedding_word[size*i:size*(i+1)]\n",
    "                inputs2 = mask_input\n",
    "                output = tf.multiply(tf.reduce_sum(tf.one_hot(inputs2,200),axis=1),inputs1)\n",
    "\n",
    "                \n",
    "                random_value = inputs1.copy()        \n",
    "                for _,index in enumerate(inputs2):\n",
    "                        for j in index:\n",
    "                                if j != -1:\n",
    "                                        prob = np.random.rand(1)[0]\n",
    "                                        if prob < 0.8:\n",
    "                                                random_value[_][j] = 0\n",
    "                                        elif prob > 0.9:\n",
    "                                                temp1 = random.sample(range(0,301),1)[0]\n",
    "                                                random_value[_][j] = temp1\n",
    "                                                \n",
    "                \n",
    "                \n",
    "                print(f'This is {i} number step')\n",
    "                with tf.device('/device:GPU:0'):\n",
    "                        if i % 20 == 0:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=256,callbacks = [early_stopping_cb],validation_data=([np.array(random_value_val),np.array(mask_input_val)],np.array(output_val)))\n",
    "                        else:\n",
    "                                model.fit([np.array(random_value),np.array(inputs2)],np.array(output,dtype = int),epochs=1,batch_size=256,callbacks = [early_stopping_cb])\n",
    "                        print(model.optimizer.lr)\n",
    "                        model.save_weights('./BERT/SMILE/Random_ZINC_L_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
